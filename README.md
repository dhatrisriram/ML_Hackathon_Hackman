
# ğŸ® Hackman â€” Hybrid HMM + DQN Hangman Solver

> UE23CS352A â€” Machine Learning Hackathon  
> **PES University | Team 7|B Section**

## ğŸ‘¥ Team Members

| Name | SRN |
|---|---|
| Namratha A | PES1UG23AM911 |
| Dhatri P Sriram | PES1UG23AM098 |
| Deepthi M | PES1UG23AM092 |
| Samruddhi Rao | PES1UG23AM915 |

---

This project develops an intelligent Hangman-playing agent using a **Hidden Markov Model (HMM)** for probabilistic language modeling and a **Deep Q-Network (DQN)** for reinforcement-based decision-making.

The aim is to maximize win rate while reducing wrong and repeated guesses, under the constraint of using only the provided 50,000-word corpus.

---

## ğŸ¯ Objective

The agent must guess hidden letters efficiently using machine learning.

**Hackathon Scoring Formula**
```
Final Score = (Success Rate Ã— 2000) - (Wrong Guesses Ã— 5) - (Repeated Guesses Ã— 2)
```

**Key Goals**
- âœ… Maximize success rate
- âŒ Minimize wrong guesses
- â™»ï¸ Avoid repeated guesses

---

## Prerequisites ##

- torch
- numpy
- pandas
- scikit-learn
- matplotlib
- seaborn
- tqdm
- hmmlearn

---

## ğŸ§  System Architecture

### **Pipeline**
```
Corpus â†’ HMM Training â†’ Letter Probability Estimation
         â†“
Hangman Environment â†” DQN Agent (Training)
         â†“
Evaluation & Score
```

### **State Representation**
- Masked word encoding
- Guessed letter vector (binary)
- Remaining lives
- HMM probability vector

### **Rewards**
| Action | Reward |
|---|---|
Correct guess | +1.0  
Wrong guess | -1.0  
Repeated guess | -0.1  
Win | +5.0  
Lose | -5.0  

### **Exploration Strategy**
- Îµ-greedy with exponential decay (1.0 â†’ 0.05)

---

## âš™ï¸ Hyperparameters

| Parameter | Value |
|---|---|
Learning Rate | 0.001  
Batch Size | 64  
Replay Buffer | 10,000  
Discount Factor (Î³) | 0.99  
Initial Îµ | 1.0 â†’ 0.05  
Optimizer | Adam  
Target Network Update | Every 200 steps  

---

## ğŸ“Š Evaluation Metrics

- âœ… Success Rate
- âŒ Avg wrong guesses/game
- ğŸ” Avg repeated guesses/game
- ğŸ§® Final hackathon score
- ğŸ“ˆ Reward convergence graph

---

## ğŸš€ Key Results

- Stable convergence observed
- Strong improvement vs frequency-based guesser
- Hybrid HMM + DQN outperformed naive RL

---

## ğŸ”® Future Scope

- Transformer-based masked token predictor
- Curriculum RL (short â†’ long words)
- Multi-agent ensemble voting
- Advanced reward shaping tuning

---

## ğŸ“œ License

Developed for **PES University â€” UE23CS352A ML Hackathon**  
ğŸ§  Academic Use Only

---

