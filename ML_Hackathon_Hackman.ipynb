{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5579efe"
      },
      "source": [
        "### üß© **Task**\n",
        "Implement a **hybrid HMM + RL system** to play *Hangman* using the corpus available at  \n",
        "`/content/corpus.txt`, and evaluate its performance on the test set in  \n",
        "`/content/test.txt`.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **Subtask ‚Äì Data Loading and Preprocessing**\n",
        "Load the corpus and test data from the given file paths, then preprocess the corpus by:\n",
        "- Filtering only valid lowercase English words.  \n",
        "- Grouping the words by their length to prepare them for HMM training.\n",
        "\n",
        "---\n",
        "\n",
        "### üí° **Reasoning**\n",
        "The corpus is used to train the Hidden Markov Model.  \n",
        "Grouping words by length helps each HMM learn the letter transition patterns specific to different word lengths, improving accuracy during gameplay.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fdfef26",
        "outputId": "786a13dd-df65-4809-8270-39bf11ce5490"
      },
      "source": [
        "import re\n",
        "\n",
        "with open('/content/corpus.txt', 'r') as f:\n",
        "    corpus = f.read().splitlines()\n",
        "\n",
        "with open('/content/test.txt', 'r') as f:\n",
        "    test_data = f.read().splitlines()\n",
        "\n",
        "corpus_by_length = {}\n",
        "for word in corpus:\n",
        "    if re.fullmatch(r'[a-z]+', word):\n",
        "        length = len(word)\n",
        "        if length not in corpus_by_length:\n",
        "            corpus_by_length[length] = []\n",
        "        corpus_by_length[length].append(word)\n",
        "\n",
        "print(\"Sample of corpus by length:\")\n",
        "for length, words in list(corpus_by_length.items())[:5]:\n",
        "    print(f\"Length {length}: {words[:10]}...\")\n",
        "\n",
        "print(\"\\nSample of test data:\")\n",
        "print(test_data[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample of corpus by length:\n",
            "Length 11: ['suburbanize', 'consonantly', 'placentitis', 'camaldolite', 'teutomaniac', 'affirmation', 'dearomatize', 'anhemolytic', 'subcategory', 'plumigerous']...\n",
            "Length 6: ['asmack', 'higgle', 'kulang', 'chandu', 'pursue', 'maumet', 'tiriba', 'leaver', 'unhewn', 'exomis']...\n",
            "Length 9: ['hypotypic', 'cacomelia', 'thicklips', 'yellowcup', 'rancorous', 'sovietist', 'megascope', 'unplaited', 'unfroward', 'pinckneya']...\n",
            "Length 16: ['promoderationist', 'galactophlebitis', 'tubulibranchiata', 'collodionization', 'unpardonableness', 'unproportionally', 'astrophotography', 'anaerobiotically', 'palaeopsychology', 'stereometrically']...\n",
            "Length 14: ['philatelically', 'cinematography', 'highfalutinism', 'autoagglutinin', 'cosmopolitanly', 'extemporaneous', 'thyroidization', 'chloridellidae', 'monotelephonic', 'metabisulphite']...\n",
            "\n",
            "Sample of test data:\n",
            "['marmar', 'janet', 'dentistical', 'troveless', 'unnotify', 'gastrostenosis', 'preaffiliation', 'obpyriform', 'veratrinize', 'protection']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e8f3b3c"
      },
      "source": [
        "### üß© **HMM Component Implementation**\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è **Subtask**\n",
        "Implement and train the **Hidden Markov Models (HMMs)** for each word length using the preprocessed corpus.  \n",
        "For each group of words with the same length, calculate the **probability distribution of letters** at each position.\n",
        "\n",
        "---\n",
        "\n",
        "### üí° **Reasoning**\n",
        "Each HMM is trained separately for a specific word length to capture letter transition and emission patterns unique to words of that size.  \n",
        "By learning letter probability distributions per position, the HMM can estimate the likelihood of each letter appearing in the hidden word during gameplay.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cda47e9",
        "outputId": "853987c9-33ad-4065-b362-1ab9acc5c06b"
      },
      "source": [
        "import string\n",
        "\n",
        "hmm_probabilities = {}\n",
        "\n",
        "for length, words in corpus_by_length.items():\n",
        "    if not words:\n",
        "        continue\n",
        "\n",
        "    letter_counts = {i: {letter: 0 for letter in string.ascii_lowercase} for i in range(length)}\n",
        "\n",
        "    for word in words:\n",
        "        for i, letter in enumerate(word):\n",
        "            if 0 <= i < length and letter in string.ascii_lowercase:\n",
        "                letter_counts[i][letter] += 1\n",
        "\n",
        "    probabilities = {i: {} for i in range(length)}\n",
        "    total_words_at_length = len(words)\n",
        "    for i in range(length):\n",
        "        for letter in string.ascii_lowercase:\n",
        "            probabilities[i][letter] = letter_counts[i][letter] / total_words_at_length\n",
        "\n",
        "    hmm_probabilities[length] = probabilities\n",
        "\n",
        "print(\"Sample of HMM probabilities by length:\")\n",
        "for length, probs in list(hmm_probabilities.items())[:3]:\n",
        "    print(f\"Length {length}:\")\n",
        "    for pos, letter_probs in list(probs.items())[:2]:\n",
        "        print(f\"  Position {pos}: {list(letter_probs.items())[:5]}...\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample of HMM probabilities by length:\n",
            "Length 11:\n",
            "  Position 0: [('a', 0.0748349229640499), ('b', 0.03173147468818782), ('c', 0.09446074834922964), ('d', 0.04548789435069699), ('e', 0.036867204695524576)]...\n",
            "  Position 1: [('a', 0.10436537050623625), ('b', 0.006969919295671314), ('c', 0.02127659574468085), ('d', 0.00586940572267058), ('e', 0.14325018341892884)]...\n",
            "Length 6:\n",
            "  Position 0: [('a', 0.07723035952063914), ('b', 0.07057256990679095), ('c', 0.08122503328894808), ('d', 0.044207723035952065), ('e', 0.03462050599201065)]...\n",
            "  Position 1: [('a', 0.17762982689747003), ('b', 0.006657789613848202), ('c', 0.021837549933422105), ('d', 0.005858854860186418), ('e', 0.1430093209054594)]...\n",
            "Length 9:\n",
            "  Position 0: [('a', 0.07396493296007073), ('b', 0.04994843082363341), ('c', 0.08368940621776927), ('d', 0.048475025784588184), ('e', 0.038455871519080594)]...\n",
            "  Position 1: [('a', 0.12715485486960365), ('b', 0.0055989391483718875), ('c', 0.020775011050537792), ('d', 0.0072196846913216445), ('e', 0.14616177987328716)]...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa27b28b"
      },
      "source": [
        "## üß© **Evaluation**\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è **Subtask**\n",
        "Evaluate the trained **Hangman agent** on the test set using the provided scoring formula to assess its overall performance.  \n",
        "For each test word, simulate a full game where the agent guesses letters according to its learned policy.\n",
        "\n",
        "---\n",
        "\n",
        "### üí° **Reasoning**\n",
        "The trained agent is tested on unseen words from the test set.  \n",
        "For each word:\n",
        "- The game is simulated step by step using the agent‚Äôs strategy.  \n",
        "- Rewards and penalties are assigned based on correct and incorrect guesses.  \n",
        "- The final **score** is calculated using the given formula:  \n",
        "  \\[\n",
        "  \\text{Final Score} = (\\text{Success Rate} \\times 2000) - (5 \\times \\text{Total Wrong Guesses}) - (2 \\times \\text{Repeated Guesses})\n",
        "  \\]  \n",
        "\n",
        "The final results include:\n",
        "- **Total score** across all games  \n",
        "- **Average score per game**  \n",
        "- **Success rate** (percentage of words correctly guessed)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d7b24f0",
        "outputId": "75fdfc10-64c0-42c0-b76c-601146cdfed7"
      },
      "source": [
        "import string\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "\n",
        "class HangmanEnv:\n",
        "    def __init__(self, corpus_by_length, hmm_probabilities, max_lives=6):\n",
        "        self.corpus_by_length = corpus_by_length\n",
        "        self.hmm_probabilities = hmm_probabilities\n",
        "        self.max_lives = max_lives\n",
        "        self.word = None\n",
        "        self.masked_word = None\n",
        "        self.guessed_letters = set()\n",
        "        self.lives_left = self.max_lives\n",
        "        self.word_length = None\n",
        "        self.current_hmm_probs = None\n",
        "\n",
        "    def reset(self):\n",
        "        self.word_length = random.choice(list(self.corpus_by_length.keys()))\n",
        "        self.word = random.choice(self.corpus_by_length[self.word_length])\n",
        "        self.masked_word = [\"_\"] * self.word_length\n",
        "        self.guessed_letters = set()\n",
        "        self.lives_left = self.max_lives\n",
        "        self.current_hmm_probs = self.hmm_probabilities.get(self.word_length, None)\n",
        "        return self._get_state()\n",
        "\n",
        "    def reset_for_eval(self, word):\n",
        "        self.word = word\n",
        "        self.word_length = len(word)\n",
        "        self.masked_word = [\"_\"] * self.word_length\n",
        "        self.guessed_letters = set()\n",
        "        self.lives_left = self.max_lives\n",
        "        self.current_hmm_probs = self.hmm_probabilities.get(self.word_length, None)\n",
        "        return self._get_state()\n",
        "\n",
        "    def step(self, action):\n",
        "        guessed_letter = action.lower()\n",
        "        if guessed_letter not in string.ascii_lowercase or guessed_letter in self.guessed_letters:\n",
        "            reward = -0.1\n",
        "            done = False\n",
        "        else:\n",
        "            self.guessed_letters.add(guessed_letter)\n",
        "            reward = 0\n",
        "            done = False\n",
        "            letter_found = False\n",
        "            for i, letter in enumerate(self.word):\n",
        "                if letter == guessed_letter:\n",
        "                    self.masked_word[i] = letter\n",
        "                    reward = 1\n",
        "                    letter_found = True\n",
        "            if not letter_found:\n",
        "                self.lives_left -= 1\n",
        "                reward = -1\n",
        "            if \"_\" not in self.masked_word:\n",
        "                done = True\n",
        "                reward = 5\n",
        "            elif self.lives_left <= 0:\n",
        "                done = True\n",
        "                reward = -5\n",
        "        return self._get_state(), reward, done, {}\n",
        "\n",
        "    def _get_state(self):\n",
        "        return {\n",
        "            \"masked_word\": \"\".join(self.masked_word),\n",
        "            \"guessed_letters\": sorted(list(self.guessed_letters)),\n",
        "            \"lives_left\": self.lives_left,\n",
        "            \"hmm_probs\": self.current_hmm_probs\n",
        "        }\n",
        "\n",
        "    def is_done(self):\n",
        "        return \"_\" not in self.masked_word or self.lives_left <= 0\n",
        "\n",
        "    def render(self):\n",
        "        print(f\"Word: {''.join(self.masked_word)}\")\n",
        "        print(f\"Guessed Letters: {sorted(list(self.guessed_letters))}\")\n",
        "        print(f\"Lives Left: {self.lives_left}\")\n",
        "\n",
        "env = HangmanEnv(corpus_by_length, hmm_probabilities, max_lives=6)\n",
        "\n",
        "agent = DQNAgent(state_size=state_size, action_size=action_size, seed=seed)\n",
        "agent.load_model(\"hangman_dqn_agent.pth\")\n",
        "\n",
        "total_score = 0\n",
        "correctly_guessed_words = 0\n",
        "\n",
        "print(\"\\nStarting evaluation on the test set...\")\n",
        "\n",
        "for word_to_guess in test_data:\n",
        "    state = env.reset_for_eval(word_to_guess)\n",
        "    numerical_state = state_to_numerical(state, max_len=max_word_length)\n",
        "    done = False\n",
        "    incorrect_guesses = 0\n",
        "    game_won = False\n",
        "\n",
        "    while not done:\n",
        "        original_epsilon = agent.epsilon\n",
        "        agent.epsilon = 0\n",
        "        action_index = agent.choose_action(numerical_state, state[\"guessed_letters\"])\n",
        "        agent.epsilon = original_epsilon\n",
        "\n",
        "        action_letter = string.ascii_lowercase[action_index]\n",
        "        next_state, reward, done, _ = env.step(action_letter)\n",
        "\n",
        "        if action_letter not in word_to_guess and action_letter not in state[\"guessed_letters\"]:\n",
        "            incorrect_guesses += 1\n",
        "\n",
        "        if \"_\" not in next_state[\"masked_word\"]:\n",
        "            game_won = True\n",
        "\n",
        "        numerical_next_state = state_to_numerical(next_state, max_len=max_word_length)\n",
        "        state = next_state\n",
        "        numerical_state = numerical_next_state\n",
        "\n",
        "    if game_won:\n",
        "        score = 10 * (6 - incorrect_guesses)\n",
        "        correctly_guessed_words += 1\n",
        "    else:\n",
        "        score = -10\n",
        "\n",
        "    total_score += score\n",
        "\n",
        "print(\"\\nEvaluation finished.\")\n",
        "print(f\"Total words in test set: {len(test_data)}\")\n",
        "print(f\"Correctly guessed words: {correctly_guessed_words}\")\n",
        "print(f\"Success Rate: {correctly_guessed_words / len(test_data) * 100:.2f}%\")\n",
        "print(f\"Total score: {total_score}\")\n",
        "\n",
        "if len(test_data) > 0:\n",
        "    print(f\"Average score per word: {total_score / len(test_data):.2f}\")\n",
        "else:\n",
        "    print(\"No words in the test set to calculate average score.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting evaluation on the test set...\n",
            "\n",
            "Evaluation finished.\n",
            "Total words in test set: 2000\n",
            "Correctly guessed words: 376\n",
            "Success Rate: 18.80%\n",
            "Total score: -7810\n",
            "Average score per word: -3.90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60651300"
      },
      "source": [
        "## üß† **Agent Training**\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è **Subtask**\n",
        "Train the **DQN (Deep Q-Network) agent** using the Hangman environment, integrating the **HMM probabilities** as part of the agent‚Äôs state input.  \n",
        "Optimize the agent‚Äôs behavior based on the defined **reward function**, encouraging correct guesses and penalizing incorrect or repeated ones.\n",
        "\n",
        "---\n",
        "\n",
        "### üí° **Reasoning**\n",
        "The agent learns to play Hangman by interacting with the environment repeatedly.  \n",
        "Each state includes:\n",
        "- The current masked word  \n",
        "- The set of guessed letters  \n",
        "- Remaining lives  \n",
        "- HMM-derived letter probability distributions  \n",
        "\n",
        "Through training, the DQN updates its Q-values to maximize cumulative reward ‚Äî learning which letters to guess in different situations to achieve higher success rates.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è **Define Hyperparameter Search Space**\n",
        "\n",
        "---\n",
        "\n",
        "### üß© **Subtask**\n",
        "Define the **range of hyperparameters** to explore for tuning the DQN agent.  \n",
        "This includes parameters like learning rate, batch size, discount factor (Œ≥), and exploration rate (Œµ).\n",
        "\n",
        "---\n",
        "\n",
        "### üí° **Reasoning**\n",
        "Create a list of hyperparameter dictionaries to represent the search space for **random search** or **grid search**.  \n",
        "The chosen ranges should reflect common best practices while being appropriate for the Hangman problem, ensuring the model is neither underfitted nor overfitted.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5c52671",
        "outputId": "01c65926-a1af-44d4-b052-4e8b8942cc6a"
      },
      "source": [
        "hyperparameter_combinations = [\n",
        "    {\n",
        "        'buffer_size': int(1e5),\n",
        "        'batch_size': 64,\n",
        "        'gamma': 0.99,\n",
        "        'lr': 5e-4,\n",
        "        'update_every': 4,\n",
        "        'epsilon_decay': 0.995,\n",
        "        'epsilon_min': 0.01,\n",
        "        'fc1_units': 64,\n",
        "        'fc2_units': 64,\n",
        "        'n_episodes': 5000,\n",
        "        'target_update_freq': 100\n",
        "    },\n",
        "    {\n",
        "        'buffer_size': int(5e4),\n",
        "        'batch_size': 32,\n",
        "        'gamma': 0.95,\n",
        "        'lr': 1e-3,\n",
        "        'update_every': 10,\n",
        "        'epsilon_decay': 0.99,\n",
        "        'epsilon_min': 0.05,\n",
        "        'fc1_units': 64,\n",
        "        'fc2_units': 64,\n",
        "        'n_episodes': 5000,\n",
        "        'target_update_freq': 50\n",
        "    },\n",
        "    {\n",
        "        'buffer_size': int(2e5),\n",
        "        'batch_size': 128,\n",
        "        'gamma': 0.999,\n",
        "        'lr': 1e-4,\n",
        "        'update_every': 1,\n",
        "        'epsilon_decay': 0.998,\n",
        "        'epsilon_min': 0.005,\n",
        "        'fc1_units': 128,\n",
        "        'fc2_units': 128,\n",
        "        'n_episodes': 5000,\n",
        "        'target_update_freq': 200\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"Defined hyperparameter combinations for tuning:\")\n",
        "for i, hp_set in enumerate(hyperparameter_combinations):\n",
        "    print(f\"Set {i+1}: {hp_set}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined hyperparameter combinations for tuning:\n",
            "Set 1: {'buffer_size': 100000, 'batch_size': 64, 'gamma': 0.99, 'lr': 0.0005, 'update_every': 4, 'epsilon_decay': 0.995, 'epsilon_min': 0.01, 'fc1_units': 64, 'fc2_units': 64, 'n_episodes': 5000, 'target_update_freq': 100}\n",
            "Set 2: {'buffer_size': 50000, 'batch_size': 32, 'gamma': 0.95, 'lr': 0.001, 'update_every': 10, 'epsilon_decay': 0.99, 'epsilon_min': 0.05, 'fc1_units': 64, 'fc2_units': 64, 'n_episodes': 5000, 'target_update_freq': 50}\n",
            "Set 3: {'buffer_size': 200000, 'batch_size': 128, 'gamma': 0.999, 'lr': 0.0001, 'update_every': 1, 'epsilon_decay': 0.998, 'epsilon_min': 0.005, 'fc1_units': 128, 'fc2_units': 128, 'n_episodes': 5000, 'target_update_freq': 200}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üí° **Reasoning**\n",
        "Train the **DQN agent** by allowing it to interact repeatedly with the **Hangman environment**.  \n",
        "Each state is represented as a **numerical vector** that includes:\n",
        "- The masked word  \n",
        "- The set of guessed letters  \n",
        "- Remaining lives  \n",
        "- The HMM probabilities corresponding to the current word‚Äôs length  \n",
        "\n",
        "Through continuous interaction, the agent learns to choose the best possible action ‚Äî i.e., which letter to guess next ‚Äî based on the **rewards** it receives for correct or incorrect guesses.\n"
      ],
      "metadata": {
        "id": "5JAhYRm_K9Pa"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38aea348",
        "outputId": "51bcd8c7-89a7-42dd-9e0a-58fcced13092"
      },
      "source": [
        "import numpy as np\n",
        "import string\n",
        "import torch\n",
        "\n",
        "env = HangmanEnv(corpus_by_length, hmm_probabilities, max_lives=6)\n",
        "\n",
        "max_word_length = max(corpus_by_length.keys()) if corpus_by_length else 24\n",
        "masked_word_representation_size = max_word_length * (len(string.ascii_lowercase) + 1)\n",
        "guessed_letters_representation_size = len(string.ascii_lowercase)\n",
        "lives_left_representation_size = 1\n",
        "hmm_probs_representation_size = max_word_length * len(string.ascii_lowercase)\n",
        "\n",
        "state_size = (\n",
        "    masked_word_representation_size +\n",
        "    guessed_letters_representation_size +\n",
        "    lives_left_representation_size +\n",
        "    hmm_probs_representation_size\n",
        ")\n",
        "action_size = len(string.ascii_lowercase)\n",
        "\n",
        "print(f\"Determined state size: {state_size}\")\n",
        "print(f\"Action size: {action_size}\")\n",
        "\n",
        "def state_to_numerical(state, max_len):\n",
        "    masked_word_str = state[\"masked_word\"]\n",
        "    guessed_letters = set(state[\"guessed_letters\"])\n",
        "    lives_left = state[\"lives_left\"]\n",
        "    hmm_probs = state[\"hmm_probs\"]\n",
        "    masked_word_vec = np.zeros(max_len * (len(string.ascii_lowercase) + 1))\n",
        "    letter_to_idx = {letter: i for i, letter in enumerate(string.ascii_lowercase)}\n",
        "    letter_to_idx['_'] = len(string.ascii_lowercase)\n",
        "    for i in range(max_len):\n",
        "        if i < len(masked_word_str):\n",
        "            char = masked_word_str[i]\n",
        "            if char in letter_to_idx:\n",
        "                masked_word_vec[i * (len(string.ascii_lowercase) + 1) + letter_to_idx[char]] = 1\n",
        "        else:\n",
        "            masked_word_vec[i * (len(string.ascii_lowercase) + 1) + letter_to_idx['_']] = 1\n",
        "    guessed_letters_vec = np.zeros(len(string.ascii_lowercase))\n",
        "    for letter in guessed_letters:\n",
        "        if letter in letter_to_idx:\n",
        "            guessed_letters_vec[letter_to_idx[letter]] = 1\n",
        "    lives_left_vec = np.array([lives_left])\n",
        "    hmm_probs_vec = np.zeros(max_len * len(string.ascii_lowercase))\n",
        "    if hmm_probs:\n",
        "        for pos in range(max_len):\n",
        "            if pos in hmm_probs:\n",
        "                for letter, prob in hmm_probs[pos].items():\n",
        "                    if letter in letter_to_idx:\n",
        "                        hmm_probs_vec[pos * len(string.ascii_lowercase) + letter_to_idx[letter]] = prob\n",
        "    numerical_state = np.concatenate([\n",
        "        masked_word_vec,\n",
        "        guessed_letters_vec,\n",
        "        lives_left_vec,\n",
        "        hmm_probs_vec\n",
        "    ])\n",
        "    return numerical_state\n",
        "\n",
        "seed = 42\n",
        "agent = DQNAgent(state_size=state_size, action_size=action_size, seed=seed)\n",
        "\n",
        "n_episodes = 20000\n",
        "target_update_freq = 100\n",
        "\n",
        "scores = []\n",
        "for i_episode in range(1, n_episodes + 1):\n",
        "    state = env.reset()\n",
        "    score = 0\n",
        "    done = False\n",
        "    numerical_state = state_to_numerical(state, max_len=max_word_length)\n",
        "    while not done:\n",
        "        action_index = agent.choose_action(numerical_state, state[\"guessed_letters\"])\n",
        "        action_letter = string.ascii_lowercase[action_index]\n",
        "        next_state, reward, done, _ = env.step(action_letter)\n",
        "        score += reward\n",
        "        numerical_next_state = state_to_numerical(next_state, max_len=max_word_length)\n",
        "        agent.step(numerical_state, action_index, reward, numerical_next_state, done)\n",
        "        state = next_state\n",
        "        numerical_state = numerical_next_state\n",
        "        if i_episode % target_update_freq == 0:\n",
        "            agent.update_target_network()\n",
        "    scores.append(score)\n",
        "    if i_episode % 100 == 0:\n",
        "        print(f'Episode {i_episode}/{n_episodes}, Average Score: {np.mean(scores[-100:]):.2f}, Epsilon: {agent.epsilon:.2f}')\n",
        "\n",
        "agent.save_model(\"hangman_dqn_agent.pth\")\n",
        "print(\"Trained agent model saved.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Determined state size: 1299\n",
            "Action size: 26\n",
            "Episode 100/20000, Average Score: -4.68, Epsilon: 0.28\n",
            "Episode 200/20000, Average Score: -2.22, Epsilon: 0.06\n",
            "Episode 300/20000, Average Score: 0.52, Epsilon: 0.01\n",
            "Episode 400/20000, Average Score: -0.16, Epsilon: 0.01\n",
            "Episode 500/20000, Average Score: 0.60, Epsilon: 0.01\n",
            "Episode 600/20000, Average Score: -0.11, Epsilon: 0.01\n",
            "Episode 700/20000, Average Score: 0.05, Epsilon: 0.01\n",
            "Episode 800/20000, Average Score: 1.46, Epsilon: 0.01\n",
            "Episode 900/20000, Average Score: -0.82, Epsilon: 0.01\n",
            "Episode 1000/20000, Average Score: -0.24, Epsilon: 0.01\n",
            "Episode 1100/20000, Average Score: 1.43, Epsilon: 0.01\n",
            "Episode 1200/20000, Average Score: 1.94, Epsilon: 0.01\n",
            "Episode 1300/20000, Average Score: 1.48, Epsilon: 0.01\n",
            "Episode 1400/20000, Average Score: 0.41, Epsilon: 0.01\n",
            "Episode 1500/20000, Average Score: 0.79, Epsilon: 0.01\n",
            "Episode 1600/20000, Average Score: 1.48, Epsilon: 0.01\n",
            "Episode 1700/20000, Average Score: 1.23, Epsilon: 0.01\n",
            "Episode 1800/20000, Average Score: 1.01, Epsilon: 0.01\n",
            "Episode 1900/20000, Average Score: 0.30, Epsilon: 0.01\n",
            "Episode 2000/20000, Average Score: 2.88, Epsilon: 0.01\n",
            "Episode 2100/20000, Average Score: 1.24, Epsilon: 0.01\n",
            "Episode 2200/20000, Average Score: 0.31, Epsilon: 0.01\n",
            "Episode 2300/20000, Average Score: 1.20, Epsilon: 0.01\n",
            "Episode 2400/20000, Average Score: 2.51, Epsilon: 0.01\n",
            "Episode 2500/20000, Average Score: 2.28, Epsilon: 0.01\n",
            "Episode 2600/20000, Average Score: 2.38, Epsilon: 0.01\n",
            "Episode 2700/20000, Average Score: 2.53, Epsilon: 0.01\n",
            "Episode 2800/20000, Average Score: 3.68, Epsilon: 0.01\n",
            "Episode 2900/20000, Average Score: 1.77, Epsilon: 0.01\n",
            "Episode 3000/20000, Average Score: 1.87, Epsilon: 0.01\n",
            "Episode 3100/20000, Average Score: 2.65, Epsilon: 0.01\n",
            "Episode 3200/20000, Average Score: 2.22, Epsilon: 0.01\n",
            "Episode 3300/20000, Average Score: 2.77, Epsilon: 0.01\n",
            "Episode 3400/20000, Average Score: 1.09, Epsilon: 0.01\n",
            "Episode 3500/20000, Average Score: 1.44, Epsilon: 0.01\n",
            "Episode 3600/20000, Average Score: 0.79, Epsilon: 0.01\n",
            "Episode 3700/20000, Average Score: 2.33, Epsilon: 0.01\n",
            "Episode 3800/20000, Average Score: 1.52, Epsilon: 0.01\n",
            "Episode 3900/20000, Average Score: 3.34, Epsilon: 0.01\n",
            "Episode 4000/20000, Average Score: 1.67, Epsilon: 0.01\n",
            "Episode 4100/20000, Average Score: 1.69, Epsilon: 0.01\n",
            "Episode 4200/20000, Average Score: 1.67, Epsilon: 0.01\n",
            "Episode 4300/20000, Average Score: 0.48, Epsilon: 0.01\n",
            "Episode 4400/20000, Average Score: 1.24, Epsilon: 0.01\n",
            "Episode 4500/20000, Average Score: 1.15, Epsilon: 0.01\n",
            "Episode 4600/20000, Average Score: 0.18, Epsilon: 0.01\n",
            "Episode 4700/20000, Average Score: -0.40, Epsilon: 0.01\n",
            "Episode 4800/20000, Average Score: 1.92, Epsilon: 0.01\n",
            "Episode 4900/20000, Average Score: 3.45, Epsilon: 0.01\n",
            "Episode 5000/20000, Average Score: 2.22, Epsilon: 0.01\n",
            "Episode 5100/20000, Average Score: 1.52, Epsilon: 0.01\n",
            "Episode 5200/20000, Average Score: 1.70, Epsilon: 0.01\n",
            "Episode 5300/20000, Average Score: 1.80, Epsilon: 0.01\n",
            "Episode 5400/20000, Average Score: 2.21, Epsilon: 0.01\n",
            "Episode 5500/20000, Average Score: 1.66, Epsilon: 0.01\n",
            "Episode 5600/20000, Average Score: 2.20, Epsilon: 0.01\n",
            "Episode 5700/20000, Average Score: 2.93, Epsilon: 0.01\n",
            "Episode 5800/20000, Average Score: 2.54, Epsilon: 0.01\n",
            "Episode 5900/20000, Average Score: 1.65, Epsilon: 0.01\n",
            "Episode 6000/20000, Average Score: 1.88, Epsilon: 0.01\n",
            "Episode 6100/20000, Average Score: 0.64, Epsilon: 0.01\n",
            "Episode 6200/20000, Average Score: 1.74, Epsilon: 0.01\n",
            "Episode 6300/20000, Average Score: 0.95, Epsilon: 0.01\n",
            "Episode 6400/20000, Average Score: 1.85, Epsilon: 0.01\n",
            "Episode 6500/20000, Average Score: 3.64, Epsilon: 0.01\n",
            "Episode 6600/20000, Average Score: 2.05, Epsilon: 0.01\n",
            "Episode 6700/20000, Average Score: 0.97, Epsilon: 0.01\n",
            "Episode 6800/20000, Average Score: 2.16, Epsilon: 0.01\n",
            "Episode 6900/20000, Average Score: -0.01, Epsilon: 0.01\n",
            "Episode 7000/20000, Average Score: 0.19, Epsilon: 0.01\n",
            "Episode 7100/20000, Average Score: 1.48, Epsilon: 0.01\n",
            "Episode 7200/20000, Average Score: 2.36, Epsilon: 0.01\n",
            "Episode 7300/20000, Average Score: 0.85, Epsilon: 0.01\n",
            "Episode 7400/20000, Average Score: 2.67, Epsilon: 0.01\n",
            "Episode 7500/20000, Average Score: 4.07, Epsilon: 0.01\n",
            "Episode 7600/20000, Average Score: 3.32, Epsilon: 0.01\n",
            "Episode 7700/20000, Average Score: 3.55, Epsilon: 0.01\n",
            "Episode 7800/20000, Average Score: 0.72, Epsilon: 0.01\n",
            "Episode 7900/20000, Average Score: 1.34, Epsilon: 0.01\n",
            "Episode 8000/20000, Average Score: 4.01, Epsilon: 0.01\n",
            "Episode 8100/20000, Average Score: -0.25, Epsilon: 0.01\n",
            "Episode 8200/20000, Average Score: 3.51, Epsilon: 0.01\n",
            "Episode 8300/20000, Average Score: 1.93, Epsilon: 0.01\n",
            "Episode 8400/20000, Average Score: 1.07, Epsilon: 0.01\n",
            "Episode 8500/20000, Average Score: 1.58, Epsilon: 0.01\n",
            "Episode 8600/20000, Average Score: 2.84, Epsilon: 0.01\n",
            "Episode 8700/20000, Average Score: 1.74, Epsilon: 0.01\n",
            "Episode 8800/20000, Average Score: 0.49, Epsilon: 0.01\n",
            "Episode 8900/20000, Average Score: 2.55, Epsilon: 0.01\n",
            "Episode 9000/20000, Average Score: 1.59, Epsilon: 0.01\n",
            "Episode 9100/20000, Average Score: 2.08, Epsilon: 0.01\n",
            "Episode 9200/20000, Average Score: 0.82, Epsilon: 0.01\n",
            "Episode 9300/20000, Average Score: 2.70, Epsilon: 0.01\n",
            "Episode 9400/20000, Average Score: 1.21, Epsilon: 0.01\n",
            "Episode 9500/20000, Average Score: 1.63, Epsilon: 0.01\n",
            "Episode 9600/20000, Average Score: 1.16, Epsilon: 0.01\n",
            "Episode 9700/20000, Average Score: 3.03, Epsilon: 0.01\n",
            "Episode 9800/20000, Average Score: 1.04, Epsilon: 0.01\n",
            "Episode 9900/20000, Average Score: 2.56, Epsilon: 0.01\n",
            "Episode 10000/20000, Average Score: -0.09, Epsilon: 0.01\n",
            "Episode 10100/20000, Average Score: 1.43, Epsilon: 0.01\n",
            "Episode 10200/20000, Average Score: 2.49, Epsilon: 0.01\n",
            "Episode 10300/20000, Average Score: 2.66, Epsilon: 0.01\n",
            "Episode 10400/20000, Average Score: 2.75, Epsilon: 0.01\n",
            "Episode 10500/20000, Average Score: 1.59, Epsilon: 0.01\n",
            "Episode 10600/20000, Average Score: 2.31, Epsilon: 0.01\n",
            "Episode 10700/20000, Average Score: -0.12, Epsilon: 0.01\n",
            "Episode 10800/20000, Average Score: 0.43, Epsilon: 0.01\n",
            "Episode 10900/20000, Average Score: 0.99, Epsilon: 0.01\n",
            "Episode 11000/20000, Average Score: 1.88, Epsilon: 0.01\n",
            "Episode 11100/20000, Average Score: 1.78, Epsilon: 0.01\n",
            "Episode 11200/20000, Average Score: 3.20, Epsilon: 0.01\n",
            "Episode 11300/20000, Average Score: 1.03, Epsilon: 0.01\n",
            "Episode 11400/20000, Average Score: 1.68, Epsilon: 0.01\n",
            "Episode 11500/20000, Average Score: 1.70, Epsilon: 0.01\n",
            "Episode 11600/20000, Average Score: 4.16, Epsilon: 0.01\n",
            "Episode 11700/20000, Average Score: 1.32, Epsilon: 0.01\n",
            "Episode 11800/20000, Average Score: 1.40, Epsilon: 0.01\n",
            "Episode 11900/20000, Average Score: 0.85, Epsilon: 0.01\n",
            "Episode 12000/20000, Average Score: 4.14, Epsilon: 0.01\n",
            "Episode 12100/20000, Average Score: 2.41, Epsilon: 0.01\n",
            "Episode 12200/20000, Average Score: 1.55, Epsilon: 0.01\n",
            "Episode 12300/20000, Average Score: 2.46, Epsilon: 0.01\n",
            "Episode 12400/20000, Average Score: 2.43, Epsilon: 0.01\n",
            "Episode 12500/20000, Average Score: 2.86, Epsilon: 0.01\n",
            "Episode 12600/20000, Average Score: 2.50, Epsilon: 0.01\n",
            "Episode 12700/20000, Average Score: 1.83, Epsilon: 0.01\n",
            "Episode 12800/20000, Average Score: 2.26, Epsilon: 0.01\n",
            "Episode 12900/20000, Average Score: 2.07, Epsilon: 0.01\n",
            "Episode 13000/20000, Average Score: 2.25, Epsilon: 0.01\n",
            "Episode 13100/20000, Average Score: 2.12, Epsilon: 0.01\n",
            "Episode 13200/20000, Average Score: 1.75, Epsilon: 0.01\n",
            "Episode 13300/20000, Average Score: 1.89, Epsilon: 0.01\n",
            "Episode 13400/20000, Average Score: 3.29, Epsilon: 0.01\n",
            "Episode 13500/20000, Average Score: 0.94, Epsilon: 0.01\n",
            "Episode 13600/20000, Average Score: 1.97, Epsilon: 0.01\n",
            "Episode 13700/20000, Average Score: 1.18, Epsilon: 0.01\n",
            "Episode 13800/20000, Average Score: 1.20, Epsilon: 0.01\n",
            "Episode 13900/20000, Average Score: 3.99, Epsilon: 0.01\n",
            "Episode 14000/20000, Average Score: 0.71, Epsilon: 0.01\n",
            "Episode 14100/20000, Average Score: 1.66, Epsilon: 0.01\n",
            "Episode 14200/20000, Average Score: 1.80, Epsilon: 0.01\n",
            "Episode 14300/20000, Average Score: 3.68, Epsilon: 0.01\n",
            "Episode 14400/20000, Average Score: 1.30, Epsilon: 0.01\n",
            "Episode 14500/20000, Average Score: 2.50, Epsilon: 0.01\n",
            "Episode 14600/20000, Average Score: 3.32, Epsilon: 0.01\n",
            "Episode 14700/20000, Average Score: 2.74, Epsilon: 0.01\n",
            "Episode 14800/20000, Average Score: 3.51, Epsilon: 0.01\n",
            "Episode 14900/20000, Average Score: 3.32, Epsilon: 0.01\n",
            "Episode 15000/20000, Average Score: 0.02, Epsilon: 0.01\n",
            "Episode 15100/20000, Average Score: 1.88, Epsilon: 0.01\n",
            "Episode 15200/20000, Average Score: 4.32, Epsilon: 0.01\n",
            "Episode 15300/20000, Average Score: 0.47, Epsilon: 0.01\n",
            "Episode 15400/20000, Average Score: 2.91, Epsilon: 0.01\n",
            "Episode 15500/20000, Average Score: 0.38, Epsilon: 0.01\n",
            "Episode 15600/20000, Average Score: 0.52, Epsilon: 0.01\n",
            "Episode 15700/20000, Average Score: 2.04, Epsilon: 0.01\n",
            "Episode 15800/20000, Average Score: 3.27, Epsilon: 0.01\n",
            "Episode 15900/20000, Average Score: 1.65, Epsilon: 0.01\n",
            "Episode 16000/20000, Average Score: 1.77, Epsilon: 0.01\n",
            "Episode 16100/20000, Average Score: 0.45, Epsilon: 0.01\n",
            "Episode 16200/20000, Average Score: 1.82, Epsilon: 0.01\n",
            "Episode 16300/20000, Average Score: 3.61, Epsilon: 0.01\n",
            "Episode 16400/20000, Average Score: 1.82, Epsilon: 0.01\n",
            "Episode 16500/20000, Average Score: 1.90, Epsilon: 0.01\n",
            "Episode 16600/20000, Average Score: 1.92, Epsilon: 0.01\n",
            "Episode 16700/20000, Average Score: 3.47, Epsilon: 0.01\n",
            "Episode 16800/20000, Average Score: 2.24, Epsilon: 0.01\n",
            "Episode 16900/20000, Average Score: 1.65, Epsilon: 0.01\n",
            "Episode 17000/20000, Average Score: 2.30, Epsilon: 0.01\n",
            "Episode 17100/20000, Average Score: 1.69, Epsilon: 0.01\n",
            "Episode 17200/20000, Average Score: 2.49, Epsilon: 0.01\n",
            "Episode 17300/20000, Average Score: 1.27, Epsilon: 0.01\n",
            "Episode 17400/20000, Average Score: 0.30, Epsilon: 0.01\n",
            "Episode 17500/20000, Average Score: 3.21, Epsilon: 0.01\n",
            "Episode 17600/20000, Average Score: 1.33, Epsilon: 0.01\n",
            "Episode 17700/20000, Average Score: 2.05, Epsilon: 0.01\n",
            "Episode 17800/20000, Average Score: 0.98, Epsilon: 0.01\n",
            "Episode 17900/20000, Average Score: 2.96, Epsilon: 0.01\n",
            "Episode 18000/20000, Average Score: 3.53, Epsilon: 0.01\n",
            "Episode 18100/20000, Average Score: 3.57, Epsilon: 0.01\n",
            "Episode 18200/20000, Average Score: 1.75, Epsilon: 0.01\n",
            "Episode 18300/20000, Average Score: 1.12, Epsilon: 0.01\n",
            "Episode 18400/20000, Average Score: 1.88, Epsilon: 0.01\n",
            "Episode 18500/20000, Average Score: 3.55, Epsilon: 0.01\n",
            "Episode 18600/20000, Average Score: 4.24, Epsilon: 0.01\n",
            "Episode 18700/20000, Average Score: 2.40, Epsilon: 0.01\n",
            "Episode 18800/20000, Average Score: 0.91, Epsilon: 0.01\n",
            "Episode 18900/20000, Average Score: 4.03, Epsilon: 0.01\n",
            "Episode 19000/20000, Average Score: 2.20, Epsilon: 0.01\n",
            "Episode 19100/20000, Average Score: 0.93, Epsilon: 0.01\n",
            "Episode 19200/20000, Average Score: 1.79, Epsilon: 0.01\n",
            "Episode 19300/20000, Average Score: 3.50, Epsilon: 0.01\n",
            "Episode 19400/20000, Average Score: 1.87, Epsilon: 0.01\n",
            "Episode 19500/20000, Average Score: 3.78, Epsilon: 0.01\n",
            "Episode 19600/20000, Average Score: 2.53, Epsilon: 0.01\n",
            "Episode 19700/20000, Average Score: 3.02, Epsilon: 0.01\n",
            "Episode 19800/20000, Average Score: 1.44, Epsilon: 0.01\n",
            "Episode 19900/20000, Average Score: 2.48, Epsilon: 0.01\n",
            "Episode 20000/20000, Average Score: 0.96, Epsilon: 0.01\n",
            "Trained agent model saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dbac43b"
      },
      "source": [
        "## üß† **RL Agent Implementation**\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è **Subtask**\n",
        "Implement the **DQN agent** with an **Œµ-greedy (epsilon-greedy)** exploration strategy and define the **neural network** used to approximate the Q-function.  \n",
        "The network predicts Q-values for all possible actions given the current state.\n",
        "\n",
        "---\n",
        "\n",
        "### üí° **Reasoning**\n",
        "The **DQN agent** uses a deep neural network to estimate Q-values for each possible action in a given state.  \n",
        "During training, it follows an **Œµ-greedy strategy** ‚Äî  \n",
        "- With probability **Œµ**, it explores by choosing a random action.  \n",
        "- With probability **1 - Œµ**, it exploits by choosing the action with the highest predicted Q-value.  \n",
        "\n",
        "This approach helps the agent effectively balance **exploration** (trying new actions) and **exploitation** (using what it has learned) to improve its decision-making over time.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "770caa62"
      },
      "source": [
        "Sure üëç ‚Äî here‚Äôs your **cleaned version** of the DQN agent code with **only essential comments kept** (no redundancy, clear and professional):\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "# Q-Network\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64, fc3_units=None):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
        "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
        "        if fc3_units is not None:\n",
        "            self.fc3 = nn.Linear(fc2_units, fc3_units)\n",
        "            self.fc4 = nn.Linear(fc3_units, action_size)\n",
        "            self.use_fc3 = True\n",
        "        else:\n",
        "            self.fc3 = nn.Linear(fc2_units, action_size)\n",
        "            self.use_fc3 = False\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        if self.use_fc3:\n",
        "            x = torch.relu(self.fc3(x))\n",
        "            return self.fc4(x)\n",
        "        else:\n",
        "            return self.fc3(x)\n",
        "\n",
        "\n",
        "BUFFER_SIZE = int(1e5)\n",
        "BATCH_SIZE = 64\n",
        "GAMMA = 0.99\n",
        "LR = 5e-4\n",
        "UPDATE_EVERY = 4\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size, seed, buffer_size=BUFFER_SIZE, batch_size=BATCH_SIZE, gamma=GAMMA, lr=LR, update_every=UPDATE_EVERY, epsilon_decay=0.995, epsilon_min=0.01, fc1_units=64, fc2_units=64, fc3_units=None):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "        self.qnetwork_local = QNetwork(state_size, action_size, seed, fc1_units, fc2_units, fc3_units).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.qnetwork_target = QNetwork(state_size, action_size, seed, fc1_units, fc2_units, fc3_units).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=lr)\n",
        "\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.t_step = 0\n",
        "\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma\n",
        "        self.update_every = update_every\n",
        "        self.lr = lr\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "        self.t_step = (self.t_step + 1) % self.update_every\n",
        "        if self.t_step == 0 and len(self.memory) > self.batch_size:\n",
        "            experiences = self.sample_from_memory()\n",
        "            self.learn(experiences, self.gamma)\n",
        "\n",
        "    def choose_action(self, state, guessed_letters):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.qnetwork_local.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self.qnetwork_local(state)\n",
        "        self.qnetwork_local.train()\n",
        "\n",
        "        if random.random() > self.epsilon:\n",
        "            action_values = action_values.squeeze().cpu().numpy()\n",
        "            for letter in guessed_letters:\n",
        "                action_values[ord(letter) - ord('a')] = -float('inf')\n",
        "            return np.argmax(action_values)\n",
        "        else:\n",
        "            available_letters = [i for i in range(self.action_size) if string.ascii_lowercase[i] not in guessed_letters]\n",
        "            if available_letters:\n",
        "                return random.choice(available_letters)\n",
        "            else:\n",
        "                return random.randint(0, self.action_size - 1)\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "        loss = nn.MSELoss()(Q_expected, Q_targets)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "    def sample_from_memory(self):\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "        states = torch.from_numpy(np.vstack([e[0] for e in experiences if e is not None])).float().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        actions = torch.from_numpy(np.vstack([e[1] for e in experiences if e is not None])).long().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences if e is not None])).float().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        next_states = torch.from_numpy(np.vstack([e[3] for e in experiences if e is not None])).float().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        dones = torch.from_numpy(np.vstack([e[4] for e in experiences if e is not None]).astype(np.uint8)).float().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def update_target_network(self):\n",
        "        for target_param, local_param in zip(self.qnetwork_target.parameters(), self.qnetwork_local.parameters()):\n",
        "            target_param.data.copy_(self.lr * local_param.data + (1.0 - self.lr) * target_param.data)\n",
        "\n",
        "    def save_model(self, path):\n",
        "        torch.save(self.qnetwork_local.state_dict(), path)\n",
        "\n",
        "    def load_model(self, path):\n",
        "        self.qnetwork_local.load_state_dict(torch.load(path))\n",
        "        self.qnetwork_target.load_state_dict(torch.load(path))\n",
        "```"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ba5ae18"
      },
      "source": [
        "## üéÆ **Hangman Environment Creation**\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è **Subtask**\n",
        "Build a **custom Hangman environment** that implements the core game logic, maintains the **state representation** (masked word, guessed letters, lives left, and HMM probabilities), and defines a **reward function** to guide the RL agent‚Äôs learning.\n",
        "\n",
        "---\n",
        "\n",
        "### üí° **Reasoning**\n",
        "The Hangman environment is implemented as a **class** that manages:\n",
        "- The current **game state** (visible word progress, guessed letters, and remaining lives)  \n",
        "- **Transitions** when the agent makes a guess  \n",
        "- **Rewards** for correct, incorrect, or repeated guesses  \n",
        "\n",
        "The state vector integrates both symbolic (letters, lives) and probabilistic (HMM-based letter likelihoods) components, enabling the RL agent to learn effective guessing strategies by balancing exploration and exploitation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ac0fb18"
      },
      "source": [
        "import random\n",
        "import string\n",
        "import numpy as np\n",
        "\n",
        "class HangmanEnv:\n",
        "    def __init__(self, corpus_by_length, hmm_probabilities, max_lives=6, correct_letter_reward=1.0, incorrect_letter_reward=-1.0, win_reward=5.0, lose_reward=-5.0, repeated_letter_penalty=-0.1):\n",
        "        self.corpus_by_length = corpus_by_length\n",
        "        self.hmm_probabilities = hmm_probabilities\n",
        "        self.max_lives = max_lives\n",
        "        self.word = None\n",
        "        self.masked_word = None\n",
        "        self.guessed_letters = set()\n",
        "        self.lives_left = self.max_lives\n",
        "        self.word_length = None\n",
        "        self.current_hmm_probs = None\n",
        "        self.correct_letter_reward = correct_letter_reward\n",
        "        self.incorrect_letter_reward = incorrect_letter_reward\n",
        "        self.win_reward = win_reward\n",
        "        self.lose_reward = lose_reward\n",
        "        self.repeated_letter_penalty = repeated_letter_penalty\n",
        "\n",
        "    def reset(self):\n",
        "        self.word_length = random.choice(list(self.corpus_by_length.keys()))\n",
        "        self.word = random.choice(self.corpus_by_length[self.word_length])\n",
        "        self.masked_word = [\"_\"] * self.word_length\n",
        "        self.guessed_letters = set()\n",
        "        self.lives_left = self.max_lives\n",
        "        self.current_hmm_probs = self.hmm_probabilities.get(self.word_length, None)\n",
        "        return self._get_state()\n",
        "\n",
        "    def step(self, action):\n",
        "        guessed_letter = action.lower()\n",
        "        if guessed_letter not in string.ascii_lowercase or guessed_letter in self.guessed_letters:\n",
        "            reward = self.repeated_letter_penalty\n",
        "            done = False\n",
        "        else:\n",
        "            self.guessed_letters.add(guessed_letter)\n",
        "            reward = 0\n",
        "            done = False\n",
        "            letter_found = False\n",
        "            for i, letter in enumerate(self.word):\n",
        "                if letter == guessed_letter:\n",
        "                    self.masked_word[i] = letter\n",
        "                    reward = self.correct_letter_reward\n",
        "                    letter_found = True\n",
        "            if not letter_found:\n",
        "                self.lives_left -= 1\n",
        "                reward = self.incorrect_letter_reward\n",
        "            if \"_\" not in self.masked_word:\n",
        "                done = True\n",
        "                reward = self.win_reward\n",
        "            elif self.lives_left <= 0:\n",
        "                done = True\n",
        "                reward = self.lose_reward\n",
        "        return self._get_state(), reward, done, {}\n",
        "\n",
        "    def _get_state(self):\n",
        "        return {\n",
        "            \"masked_word\": \"\".join(self.masked_word),\n",
        "            \"guessed_letters\": sorted(list(self.guessed_letters)),\n",
        "            \"lives_left\": self.lives_left,\n",
        "            \"hmm_probs\": self.current_hmm_probs\n",
        "        }\n",
        "\n",
        "    def is_done(self):\n",
        "        return \"_\" not in self.masked_word or self.lives_left <= 0\n",
        "\n",
        "    def render(self):\n",
        "        print(f\"Word: {''.join(self.masked_word)}\")\n",
        "        print(f\"Guessed Letters: {sorted(list(self.guessed_letters))}\")\n",
        "        print(f\"Lives Left: {self.lives_left}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a676dae"
      },
      "source": [
        "# üß© Task  \n",
        "Tune the hyperparameters of the DQN agent to improve the success rate of the Hangman game  \n",
        "using the provided corpus and test data located at **\"/content/corpus.txt\"** and **\"/content/test.txt\"**.  \n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Identify Hyperparameters to Tune  \n",
        "\n",
        "### üß± Subtask  \n",
        "Identify the key hyperparameters of the DQN agent that are most likely to impact performance, such as:  \n",
        "- Learning rate (`lr`)  \n",
        "- Discount factor (`gamma`)  \n",
        "- Replay buffer size  \n",
        "- Batch size  \n",
        "- Epsilon decay rate and minimum epsilon  \n",
        "- Neural network architecture (number of layers and hidden units)  \n",
        "- Target network update frequency  \n",
        "\n",
        "---\n",
        "\n",
        "### üí° Reasoning  \n",
        "Review the DQN agent and Q-Network implementation to identify which hyperparameters  \n",
        "most strongly influence:  \n",
        "- Training stability  \n",
        "- Convergence speed  \n",
        "- The balance between exploration and exploitation during learning.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a3b6c79",
        "outputId": "17c215e1-2fa1-41d1-a369-52c750d4bd36"
      },
      "source": [
        "print(\"Identified hyperparameters and their current values:\")\n",
        "print(f\"BUFFER_SIZE: {BUFFER_SIZE}\")\n",
        "print(f\"BATCH_SIZE: {BATCH_SIZE}\")\n",
        "print(f\"GAMMA: {GAMMA}\")\n",
        "print(f\"LR: {LR}\")\n",
        "print(f\"UPDATE_EVERY: {UPDATE_EVERY}\")\n",
        "print(f\"epsilon_decay: {agent.epsilon_decay}\")\n",
        "print(f\"epsilon_min: {agent.epsilon_min}\")\n",
        "print(f\"fc1_units: {agent.qnetwork_local.fc1.in_features}\")\n",
        "print(f\"fc2_units: {agent.qnetwork_local.fc2.in_features}\")\n",
        "print(f\"n_episodes: {n_episodes}\")\n",
        "print(f\"target_update_freq: {target_update_freq}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Identified hyperparameters and their current values:\n",
            "BUFFER_SIZE: 100000\n",
            "BATCH_SIZE: 64\n",
            "GAMMA: 0.99\n",
            "LR: 0.0005\n",
            "UPDATE_EVERY: 4\n",
            "epsilon_decay: 0.995\n",
            "epsilon_min: 0.01\n",
            "fc1_units: 1299\n",
            "fc2_units: 64\n",
            "n_episodes: 20000\n",
            "target_update_freq: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50cc7240"
      },
      "source": [
        "## Define a tuning strategy\n",
        "\n",
        "### Subtask:\n",
        "Choose a method for exploring the hyperparameter space (e.g., manual search, grid search, random search).\n",
        "\n",
        "**Reasoning**:  \n",
        "Select a tuning approach that balances exploration and computational efficiency.  \n",
        "Since exhaustive grid search can be computationally expensive for deep RL, random search is chosen to efficiently sample hyperparameter combinations and identify promising configurations without training on every possible combination.\n",
        "\n",
        "\n",
        "## Implement hyperparameter variations\n",
        "\n",
        "### Subtask:\n",
        "Implement hyperparameter variations to allow easy experimentation with different hyperparameter values.\n",
        "\n",
        "**Reasoning**:  \n",
        "Encapsulate the agent training and evaluation process into a function that accepts hyperparameter values as arguments.  \n",
        "This allows systematic experimentation with different hyperparameter sets, enabling efficient tuning and comparison of results.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4ded94f"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import string\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64, fc3_units=None):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
        "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
        "        if fc3_units is not None:\n",
        "            self.fc3 = nn.Linear(fc2_units, fc3_units)\n",
        "            self.fc4 = nn.Linear(fc3_units, action_size)\n",
        "            self.use_fc3 = True\n",
        "        else:\n",
        "            self.fc3 = nn.Linear(fc2_units, action_size)\n",
        "            self.use_fc3 = False\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        if self.use_fc3:\n",
        "            x = torch.relu(self.fc3(x))\n",
        "            return self.fc4(x)\n",
        "        else:\n",
        "            return self.fc3(x)\n",
        "\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size, seed, buffer_size, batch_size, gamma, lr, update_every, epsilon_start, epsilon_decay, epsilon_min, fc1_units, fc2_units, fc3_units=None, hmm_weight=0.0, rl_weight=1.0):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(seed)\n",
        "        self.qnetwork_local = QNetwork(state_size, action_size, seed, fc1_units, fc2_units, fc3_units).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.qnetwork_target = QNetwork(state_size, action_size, seed, fc1_units, fc2_units, fc3_units).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=lr)\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.t_step = 0\n",
        "        self.epsilon = epsilon_start\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma\n",
        "        self.update_every = update_every\n",
        "        self.lr = lr\n",
        "        self.hmm_weight = hmm_weight\n",
        "        self.rl_weight = rl_weight\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "        self.t_step = (self.t_step + 1) % self.update_every\n",
        "        if self.t_step == 0:\n",
        "            if len(self.memory) > self.batch_size:\n",
        "                experiences = self.sample_from_memory()\n",
        "                self.learn(experiences, self.gamma)\n",
        "\n",
        "    def choose_action(self, state, guessed_letters):\n",
        "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.qnetwork_local.eval()\n",
        "        with torch.no_grad():\n",
        "            rl_action_values = self.qnetwork_local(state_tensor).squeeze().cpu().numpy()\n",
        "        self.qnetwork_local.train()\n",
        "\n",
        "        hmm_probs_start_idx = self.state_size - max_word_length * len(string.ascii_lowercase)\n",
        "        hmm_probs_flat = state[hmm_probs_start_idx:]\n",
        "        hmm_action_values = np.zeros(self.action_size)\n",
        "\n",
        "        if state[hmm_probs_start_idx:].sum() > 0:\n",
        "            hmm_probs_reshaped = hmm_probs_flat.reshape(max_word_length, len(string.ascii_lowercase))\n",
        "            hmm_action_values = np.sum(hmm_probs_reshaped, axis=0)\n",
        "            sum_hmm_probs_unguessed = sum(hmm_action_values[ord(letter) - ord('a')] for letter in string.ascii_lowercase if letter not in guessed_letters)\n",
        "            if sum_hmm_probs_unguessed > 0:\n",
        "                for i, letter in enumerate(string.ascii_lowercase):\n",
        "                    if letter not in guessed_letters:\n",
        "                        hmm_action_values[i] /= sum_hmm_probs_unguessed\n",
        "            else:\n",
        "                num_unguessed = len(string.ascii_lowercase) - len(guessed_letters)\n",
        "                if num_unguessed > 0:\n",
        "                    for i, letter in enumerate(string.ascii_lowercase):\n",
        "                        if letter not in guessed_letters:\n",
        "                            hmm_action_values[i] = 1.0 / num_unguessed\n",
        "\n",
        "        min_rl = np.min(rl_action_values)\n",
        "        max_rl = np.max(rl_action_values)\n",
        "        if max_rl - min_rl > 0:\n",
        "            normalized_rl_action_values = (rl_action_values - min_rl) / (max_rl - min_rl)\n",
        "        else:\n",
        "            normalized_rl_action_values = np.zeros_like(rl_action_values)\n",
        "\n",
        "        hybrid_action_values = self.rl_weight * normalized_rl_action_values + self.hmm_weight * hmm_action_values\n",
        "\n",
        "        for letter in guessed_letters:\n",
        "            hybrid_action_values[ord(letter) - ord('a')] = -float('inf')\n",
        "\n",
        "        if random.random() > self.epsilon:\n",
        "            return np.argmax(hybrid_action_values)\n",
        "        else:\n",
        "            available_letters = [i for i in range(self.action_size) if string.ascii_lowercase[i] not in guessed_letters]\n",
        "            if available_letters:\n",
        "                return random.choice(available_letters)\n",
        "            else:\n",
        "                return random.randint(0, self.action_size - 1)\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "        loss = nn.MSELoss()(Q_expected, Q_targets)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def sample_from_memory(self):\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "        states = torch.from_numpy(np.vstack([e[0] for e in experiences if e is not None])).float().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        actions = torch.from_numpy(np.vstack([e[1] for e in experiences if e is not None])).long().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences if e is not None])).float().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        next_states = torch.from_numpy(np.vstack([e[3] for e in experiences if e is not None])).float().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        dones = torch.from_numpy(np.vstack([e[4] for e in experiences if e is not None]).astype(np.uint8)).float().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def update_target_network(self):\n",
        "        for target_param, local_param in zip(self.qnetwork_target.parameters(), self.qnetwork_local.parameters()):\n",
        "            target_param.data.copy_(self.lr * local_param.data + (1.0 - self.lr) * target_param.data)\n",
        "\n",
        "    def save_model(self, path):\n",
        "        torch.save(self.qnetwork_local.state_dict(), path)\n",
        "\n",
        "    def load_model(self, path):\n",
        "        self.qnetwork_local.load_state_dict(torch.load(path))\n",
        "        self.qnetwork_target.load_state_dict(torch.load(path))\n",
        "\n",
        "\n",
        "def state_to_numerical(state, max_len):\n",
        "    masked_word_str = state[\"masked_word\"]\n",
        "    guessed_letters = set(state[\"guessed_letters\"])\n",
        "    lives_left = state[\"lives_left\"]\n",
        "    hmm_probs = state[\"hmm_probs\"]\n",
        "    masked_word_vec = np.zeros(max_len * (len(string.ascii_lowercase) + 1))\n",
        "    letter_to_idx = {letter: i for i, letter in enumerate(string.ascii_lowercase)}\n",
        "    letter_to_idx['_'] = len(string.ascii_lowercase)\n",
        "    for i in range(max_len):\n",
        "        if i < len(masked_word_str):\n",
        "            char = masked_word_str[i]\n",
        "            if char in letter_to_idx:\n",
        "                masked_word_vec[i * (len(string.ascii_lowercase) + 1) + letter_to_idx[char]] = 1\n",
        "        else:\n",
        "            masked_word_vec[i * (len(string.ascii_lowercase) + 1) + letter_to_idx['_']] = 1\n",
        "    guessed_letters_vec = np.zeros(len(string.ascii_lowercase))\n",
        "    for letter in guessed_letters:\n",
        "        if letter in letter_to_idx:\n",
        "            guessed_letters_vec[letter_to_idx[letter]] = 1\n",
        "    lives_left_vec = np.array([lives_left])\n",
        "    hmm_probs_vec = np.zeros(max_len * len(string.ascii_lowercase))\n",
        "    if hmm_probs:\n",
        "        for pos in range(max_len):\n",
        "            if pos in hmm_probs:\n",
        "                for letter, prob in hmm_probs[pos].items():\n",
        "                    if letter in letter_to_idx:\n",
        "                        hmm_probs_vec[pos * len(string.ascii_lowercase) + letter_to_idx[letter]] = prob\n",
        "    numerical_state = np.concatenate([\n",
        "        masked_word_vec,\n",
        "        guessed_letters_vec,\n",
        "        lives_left_vec,\n",
        "        hmm_probs_vec\n",
        "    ])\n",
        "    return numerical_state\n",
        "\n",
        "\n",
        "class HangmanEnv:\n",
        "    def __init__(self, corpus_by_length, hmm_probabilities, max_lives=6, correct_letter_reward=1.0, incorrect_letter_reward=-1.0, win_reward=5.0, lose_reward=-5.0, repeated_letter_penalty=-0.1):\n",
        "        self.corpus_by_length = corpus_by_length\n",
        "        self.hmm_probabilities = hmm_probabilities\n",
        "        self.max_lives = max_lives\n",
        "        self.word = None\n",
        "        self.masked_word = None\n",
        "        self.guessed_letters = set()\n",
        "        self.lives_left = self.max_lives\n",
        "        self.word_length = None\n",
        "        self.current_hmm_probs = None\n",
        "        self.correct_letter_reward = correct_letter_reward\n",
        "        self.incorrect_letter_reward = incorrect_letter_reward\n",
        "        self.win_reward = win_reward\n",
        "        self.lose_reward = lose_reward\n",
        "        self.repeated_letter_penalty = repeated_letter_penalty\n",
        "\n",
        "    def reset(self):\n",
        "        self.word_length = random.choice(list(self.corpus_by_length.keys()))\n",
        "        self.word = random.choice(self.corpus_by_length[self.word_length])\n",
        "        self.masked_word = [\"_\"] * self.word_length\n",
        "        self.guessed_letters = set()\n",
        "        self.lives_left = self.max_lives\n",
        "        self.current_hmm_probs = self.hmm_probabilities.get(self.word_length, None)\n",
        "        return self._get_state()\n",
        "\n",
        "    def reset_for_eval(self, word):\n",
        "        self.word = word\n",
        "        self.word_length = len(word)\n",
        "        self.masked_word = [\"_\"] * self.word_length\n",
        "        self.guessed_letters = set()\n",
        "        self.lives_left = self.max_lives\n",
        "        self.current_hmm_probs = self.hmm_probabilities.get(self.word_length, None)\n",
        "        return self._get_state()\n",
        "\n",
        "    def step(self, action):\n",
        "        guessed_letter = action.lower()\n",
        "        if guessed_letter not in string.ascii_lowercase or guessed_letter in self.guessed_letters:\n",
        "            reward = self.repeated_letter_penalty\n",
        "            done = False\n",
        "        else:\n",
        "            self.guessed_letters.add(guessed_letter)\n",
        "            reward = 0\n",
        "            done = False\n",
        "            letter_found = False\n",
        "            for i, letter in enumerate(self.word):\n",
        "                if letter == guessed_letter:\n",
        "                    self.masked_word[i] = letter\n",
        "                    reward = self.correct_letter_reward\n",
        "                    letter_found = True\n",
        "            if not letter_found:\n",
        "                self.lives_left -= 1\n",
        "                reward = self.incorrect_letter_reward\n",
        "            if \"_\" not in self.masked_word:\n",
        "                done = True\n",
        "                reward = self.win_reward\n",
        "            elif self.lives_left <= 0:\n",
        "                done = True\n",
        "                reward = self.lose_reward\n",
        "        return self._get_state(), reward, done, {}\n",
        "\n",
        "    def _get_state(self):\n",
        "        return {\n",
        "            \"masked_word\": \"\".join(self.masked_word),\n",
        "            \"guessed_letters\": sorted(list(self.guessed_letters)),\n",
        "            \"lives_left\": self.lives_left,\n",
        "            \"hmm_probs\": self.current_hmm_probs\n",
        "        }\n",
        "\n",
        "    def is_done(self):\n",
        "        return \"_\" not in self.masked_word or self.lives_left <= 0\n",
        "\n",
        "    def render(self):\n",
        "        print(f\"Word: {''.join(self.masked_word)}\")\n",
        "        print(f\"Guessed Letters: {sorted(list(self.guessed_letters))}\")\n",
        "        print(f\"Lives Left: {self.lives_left}\")\n",
        "\n",
        "\n",
        "def train_and_evaluate(hyperparameters, corpus_by_length, hmm_probabilities, test_data, max_word_length, seed=42):\n",
        "    buffer_size = hyperparameters['buffer_size']\n",
        "    batch_size = hyperparameters['batch_size']\n",
        "    gamma = hyperparameters['gamma']\n",
        "    lr = hyperparameters['lr']\n",
        "    update_every = hyperparameters['update_every']\n",
        "    epsilon_start = hyperparameters['epsilon_start']\n",
        "    epsilon_decay = hyperparameters['epsilon_decay']\n",
        "    epsilon_min = hyperparameters['epsilon_min']\n",
        "    fc1_units = hyperparameters['fc1_units']\n",
        "    fc2_units = hyperparameters['fc2_units']\n",
        "    fc3_units = hyperparameters.get('fc3_units', None)\n",
        "    n_episodes = hyperparameters['n_episodes']\n",
        "    target_update_freq = hyperparameters['target_update_freq']\n",
        "    hmm_weight = hyperparameters.get('hmm_weight', 0.0)\n",
        "    rl_weight = hyperparameters.get('rl_weight', 1.0)\n",
        "    correct_letter_reward = hyperparameters.get('correct_letter_reward', 1.0)\n",
        "    incorrect_letter_reward = hyperparameters.get('incorrect_letter_reward', -1.0)\n",
        "    win_reward = hyperparameters.get('win_reward', 5.0)\n",
        "    lose_reward = hyperparameters.get('lose_reward', -5.0)\n",
        "    repeated_letter_penalty = hyperparameters.get('repeated_letter_penalty', -0.1)\n",
        "\n",
        "    masked_word_representation_size = max_word_length * (len(string.ascii_lowercase) + 1)\n",
        "    guessed_letters_representation_size = len(string.ascii_lowercase)\n",
        "    lives_left_representation_size = 1\n",
        "    hmm_probs_representation_size = max_word_length * len(string.ascii_lowercase)\n",
        "    state_size = (\n",
        "        masked_word_representation_size +\n",
        "        guessed_letters_representation_size +\n",
        "        lives_left_representation_size +\n",
        "        hmm_probs_representation_size\n",
        "    )\n",
        "    action_size = len(string.ascii_lowercase)\n",
        "\n",
        "    env = HangmanEnv(corpus_by_length, hmm_probabilities, max_lives=6,\n",
        "                     correct_letter_reward=correct_letter_reward,\n",
        "                     incorrect_letter_reward=incorrect_letter_reward,\n",
        "                     win_reward=win_reward,\n",
        "                     lose_reward=lose_reward,\n",
        "                     repeated_letter_penalty=repeated_letter_penalty)\n",
        "\n",
        "    agent = DQNAgent(state_size=state_size, action_size=action_size, seed=seed,\n",
        "                     buffer_size=buffer_size, batch_size=batch_size, gamma=gamma, lr=lr,\n",
        "                     update_every=update_every, epsilon_start=epsilon_start, epsilon_decay=epsilon_decay, epsilon_min=epsilon_min,\n",
        "                     fc1_units=fc1_units, fc2_units=fc2_units, fc3_units=fc3_units,\n",
        "                     hmm_weight=hmm_weight, rl_weight=rl_weight)\n",
        "\n",
        "    scores = []\n",
        "    for i_episode in range(1, n_episodes + 1):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        done = False\n",
        "        numerical_state = state_to_numerical(state, max_len=max_word_length)\n",
        "        while not done:\n",
        "            action_index = agent.choose_action(numerical_state, state[\"guessed_letters\"])\n",
        "            action_letter = string.ascii_lowercase[action_index]\n",
        "            next_state, reward, done, _ = env.step(action_letter)\n",
        "            score += reward\n",
        "            numerical_next_state = state_to_numerical(next_state, max_len=max_word_length)\n",
        "            agent.step(numerical_state, action_index, reward, numerical_next_state, done)\n",
        "            state = next_state\n",
        "            numerical_state = numerical_next_state\n",
        "            if i_episode % target_update_freq == 0:\n",
        "                agent.update_target_network()\n",
        "        scores.append(score)\n",
        "        agent.epsilon = max(agent.epsilon_min, agent.epsilon * agent.epsilon_decay)\n",
        "        if i_episode % 1000 == 0:\n",
        "            print(f'Episode {i_episode}/{n_episodes}, Average Score: {np.mean(scores[-1000:]):.2f}, Epsilon: {agent.epsilon:.4f}')\n",
        "\n",
        "    total_score = 0\n",
        "    correctly_guessed_words = 0\n",
        "    print(\"\\nStarting evaluation on the test set...\")\n",
        "    for word_to_guess in test_data:\n",
        "        state = env.reset_for_eval(word_to_guess)\n",
        "        numerical_state = state_to_numerical(state, max_len=max_word_length)\n",
        "        done = False\n",
        "        incorrect_guesses = 0\n",
        "        game_won = False\n",
        "        while not done:\n",
        "            original_epsilon = agent.epsilon\n",
        "            agent.epsilon = 0\n",
        "            action_index = agent.choose_action(numerical_state, state[\"guessed_letters\"])\n",
        "            agent.epsilon = original_epsilon\n",
        "            action_letter = string.ascii_lowercase[action_index]\n",
        "            next_state, reward, done, _ = env.step(action_letter)\n",
        "            if action_letter not in word_to_guess and action_letter not in state[\"guessed_letters\"]:\n",
        "                incorrect_guesses += 1\n",
        "            if \"_\" not in next_state[\"masked_word\"]:\n",
        "                game_won = True\n",
        "            numerical_next_state = state_to_numerical(next_state, max_len=max_word_length)\n",
        "            state = next_state\n",
        "            numerical_state = numerical_next_state\n",
        "        if game_won:\n",
        "            score = 10 * (6 - incorrect_guesses)\n",
        "            total_score += score\n",
        "            correctly_guessed_words += 1\n",
        "        else:\n",
        "            score = -10\n",
        "            total_score += score\n",
        "\n",
        "    evaluation_results = {\n",
        "        \"total_words\": len(test_data),\n",
        "        \"correctly_guessed\": correctly_guessed_words,\n",
        "        \"success_rate\": correctly_guessed_words / len(test_data) * 100 if len(test_data) > 0 else 0,\n",
        "        \"total_score\": total_score,\n",
        "        \"average_score\": total_score / len(test_data) if len(test_data) > 0 else 0\n",
        "    }\n",
        "\n",
        "    print(\"\\nEvaluation finished.\")\n",
        "    print(f\"Correctly guessed words: {evaluation_results['correctly_guessed']}\")\n",
        "    print(f\"Success Rate: {evaluation_results['success_rate']:.2f}%\")\n",
        "    print(f\"Total score: {evaluation_results['total_score']}\")\n",
        "    print(f\"Average score per word: {evaluation_results['average_score']:.2f}\")\n",
        "\n",
        "    return evaluation_results\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20c07341"
      },
      "source": [
        "## Run hyperparameter tuning\n",
        "\n",
        "### Subtask:\n",
        "Execute the implemented tuning strategy to train and evaluate the agent with various hyperparameter combinations.\n",
        "\n",
        "**Reasoning**:\n",
        "Iterate through the defined hyperparameter combinations and train and evaluate the agent for each set, storing the results for later analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_results_list = []\n",
        "\n",
        "max_word_length = max(corpus_by_length.keys()) if corpus_by_length else 24\n",
        "\n",
        "for i, hp_set in enumerate(hyperparameter_combinations):\n",
        "    print(f\"\\n--- Training and evaluating with Hyperparameter Set {i+1} ---\")\n",
        "    print(\"Hyperparameters:\", hp_set)\n",
        "    results = train_and_evaluate(hp_set, corpus_by_length, hmm_probabilities, test_data, max_word_length)\n",
        "    evaluation_results_list.append({'hyperparameters': hp_set, 'results': results})\n",
        "\n",
        "print(\"\\n--- Summary of Hyperparameter Tuning Results ---\")\n",
        "for result_entry in evaluation_results_list:\n",
        "    print(\"\\nHyperparameters:\", result_entry['hyperparameters'])\n",
        "    print(\"Evaluation Results:\", result_entry['results'])"
      ],
      "metadata": {
        "id": "G2AEJndgOycv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_results_list = []\n",
        "\n",
        "max_word_length = max(corpus_by_length.keys()) if corpus_by_length else 24\n",
        "\n",
        "for i, hp_set in enumerate(hyperparameter_combinations):\n",
        "    print(f\"\\n--- Training and evaluating with Hyperparameter Set {i+1} ---\")\n",
        "    print(\"Hyperparameters:\", hp_set)\n",
        "    results = train_and_evaluate(hp_set, corpus_by_length, hmm_probabilities, test_data, max_word_length)\n",
        "    evaluation_results_list.append({'hyperparameters': hp_set, 'results': results})\n",
        "\n",
        "print(\"\\n--- Summary of Hyperparameter Tuning Results ---\")\n",
        "for result_entry in evaluation_results_list:\n",
        "    print(\"\\nHyperparameters:\", result_entry['hyperparameters'])\n",
        "    print(\"Evaluation Results:\", result_entry['results'])"
      ],
      "metadata": {
        "id": "Cu6Pbv52SDd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac4195f1"
      },
      "source": [
        "## Retrain the agent with new hyperparameters\n",
        "\n",
        "### Subtask:\n",
        "Retrain the agent with new hyperparameters by running the `train_and_evaluate` function with different combinations of hyperparameter values.\n",
        "\n",
        "**Reasoning**:\n",
        "Define a list of hyperparameter dictionaries and iterate through them, calling the `train_and_evaluate` function for each set and storing the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5ba7d42",
        "outputId": "1c34b935-3bef-46d2-a8f7-2cb6965724ad"
      },
      "source": [
        "hyperparameter_combinations = [\n",
        "    {\n",
        "        'buffer_size': int(1e5),\n",
        "        'batch_size': 64,\n",
        "        'gamma': 0.99,\n",
        "        'lr': 5e-4,\n",
        "        'update_every': 4,\n",
        "        'epsilon_decay': 0.995,\n",
        "        'epsilon_min': 0.01,\n",
        "        'fc1_units': 128,\n",
        "        'fc2_units': 128,\n",
        "        'n_episodes': 10000,\n",
        "        'target_update_freq': 100\n",
        "    }\n",
        "]\n",
        "\n",
        "evaluation_results_list = []\n",
        "\n",
        "max_word_length = max(corpus_by_length.keys()) if corpus_by_length else 24\n",
        "\n",
        "for i, hp_set in enumerate(hyperparameter_combinations):\n",
        "    print(f\"\\n--- Training and evaluating with Hyperparameter Set {i+1} ---\")\n",
        "    print(\"Hyperparameters:\", hp_set)\n",
        "    results = train_and_evaluate(hp_set, corpus_by_length, hmm_probabilities, test_data, max_word_length)\n",
        "    evaluation_results_list.append({'hyperparameters': hp_set, 'results': results})\n",
        "\n",
        "print(\"\\n--- Summary of Hyperparameter Tuning Results ---\")\n",
        "for result_entry in evaluation_results_list:\n",
        "    print(\"\\nHyperparameters:\", result_entry['hyperparameters'])\n",
        "    print(\"Evaluation Results:\", result_entry['results'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training and evaluating with Hyperparameter Set 1 ---\n",
            "Hyperparameters: {'buffer_size': 100000, 'batch_size': 64, 'gamma': 0.99, 'lr': 0.0005, 'update_every': 4, 'epsilon_decay': 0.995, 'epsilon_min': 0.01, 'fc1_units': 128, 'fc2_units': 128, 'n_episodes': 10000, 'target_update_freq': 100}\n",
            "Episode 1000/10000, Average Score: -0.01, Epsilon: 0.01\n",
            "Episode 2000/10000, Average Score: 0.94, Epsilon: 0.01\n",
            "Episode 3000/10000, Average Score: 0.86, Epsilon: 0.01\n",
            "Episode 4000/10000, Average Score: 1.17, Epsilon: 0.01\n",
            "Episode 5000/10000, Average Score: 1.28, Epsilon: 0.01\n",
            "Episode 6000/10000, Average Score: 1.48, Epsilon: 0.01\n",
            "Episode 7000/10000, Average Score: 1.84, Epsilon: 0.01\n",
            "Episode 8000/10000, Average Score: 1.97, Epsilon: 0.01\n",
            "Episode 9000/10000, Average Score: 2.13, Epsilon: 0.01\n",
            "Episode 10000/10000, Average Score: 2.19, Epsilon: 0.01\n",
            "\n",
            "Starting evaluation on the test set...\n",
            "\n",
            "Evaluation finished.\n",
            "Correctly guessed words: 287\n",
            "Success Rate: 14.35%\n",
            "Total score: -10520\n",
            "Average score per word: -5.26\n",
            "\n",
            "--- Summary of Hyperparameter Tuning Results ---\n",
            "\n",
            "Hyperparameters: {'buffer_size': 100000, 'batch_size': 64, 'gamma': 0.99, 'lr': 0.0005, 'update_every': 4, 'epsilon_decay': 0.995, 'epsilon_min': 0.01, 'fc1_units': 128, 'fc2_units': 128, 'n_episodes': 10000, 'target_update_freq': 100}\n",
            "Evaluation Results: {'total_words': 2000, 'correctly_guessed': 287, 'success_rate': 14.35, 'total_score': -10520, 'average_score': -5.26}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1679ba8"
      },
      "source": [
        "# üéØ Hyperparameter Analysis and Final Evaluation\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Step 1: Analyze Results and Select Best Hyperparameters\n",
        "\n",
        "### üß© Subtask:\n",
        "Analyze the evaluation results obtained from various hyperparameter combinations and identify the **best performing configuration** based on metrics such as **success rate**, **average score**, and **overall stability**.\n",
        "\n",
        "### üß† Objective:\n",
        "Determine which hyperparameter set yields the **most consistent and highest-performing Hangman agent**.\n",
        "\n",
        "---\n",
        "\n",
        "## üèÅ Step 2: Final Evaluation with Best Hyperparameters\n",
        "\n",
        "### üß© Subtask:\n",
        "Conduct a **final training and evaluation** using the best hyperparameters identified from the tuning phase.  \n",
        "Validate that the agent performs optimally and the results are **reproducible and stable**.\n",
        "\n",
        "### üí° Reasoning:\n",
        "Re-run the training process using the top-performing hyperparameter configuration to:\n",
        "- Confirm the improvement in success rate and average score.  \n",
        "- Ensure that the model generalizes well across unseen test data.  \n",
        "- Validate the robustness of the chosen hyperparameters.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **Outcome:**\n",
        "A refined DQN Hangman agent trained with optimized hyperparameters, demonstrating superior performance and reliability across evaluation metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d02cd395",
        "outputId": "c20fe8b3-9646-497a-ce40-9071b54b1d92"
      },
      "source": [
        "best_hyperparameters = {\n",
        "    'buffer_size': int(1e5),\n",
        "    'batch_size': 64,\n",
        "    'gamma': 0.99,\n",
        "    'lr': 5e-4,\n",
        "    'update_every': 4,\n",
        "    'epsilon_decay': 0.995,\n",
        "    'epsilon_min': 0.01,\n",
        "    'fc1_units': 128,\n",
        "    'fc2_units': 128,\n",
        "    'n_episodes': 5000,\n",
        "    'target_update_freq': 100\n",
        "}\n",
        "\n",
        "print(\"--- Running Final Evaluation with Best Hyperparameters ---\")\n",
        "print(\"Best Hyperparameters:\", best_hyperparameters)\n",
        "\n",
        "max_word_length = max(corpus_by_length.keys()) if corpus_by_length else 24\n",
        "\n",
        "final_evaluation_results = train_and_evaluate(\n",
        "    best_hyperparameters,\n",
        "    corpus_by_length,\n",
        "    hmm_probabilities,\n",
        "    test_data,\n",
        "    max_word_length\n",
        ")\n",
        "\n",
        "print(\"\\n--- Final Evaluation Results (Best Hyperparameters) ---\")\n",
        "print(f\"Total words in test set: {final_evaluation_results['total_words']}\")\n",
        "print(f\"Correctly guessed words: {final_evaluation_results['correctly_guessed']}\")\n",
        "print(f\"Success Rate: {final_evaluation_results['success_rate']:.2f}%\")\n",
        "print(f\"Total score: {final_evaluation_results['total_score']}\")\n",
        "print(f\"Average score per word: {final_evaluation_results['average_score']:.2f}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Running Final Evaluation with Best Hyperparameters ---\n",
            "Best Hyperparameters: {'buffer_size': 100000, 'batch_size': 64, 'gamma': 0.99, 'lr': 0.0005, 'update_every': 4, 'epsilon_decay': 0.995, 'epsilon_min': 0.01, 'fc1_units': 128, 'fc2_units': 128, 'n_episodes': 5000, 'target_update_freq': 100}\n",
            "Episode 1000/5000, Average Score: -0.01, Epsilon: 0.01\n",
            "Episode 2000/5000, Average Score: 0.94, Epsilon: 0.01\n",
            "Episode 3000/5000, Average Score: 0.86, Epsilon: 0.01\n",
            "Episode 4000/5000, Average Score: 1.17, Epsilon: 0.01\n",
            "Episode 5000/5000, Average Score: 1.28, Epsilon: 0.01\n",
            "\n",
            "Starting evaluation on the test set...\n",
            "\n",
            "Evaluation finished.\n",
            "Correctly guessed words: 275\n",
            "Success Rate: 13.75%\n",
            "Total score: -11040\n",
            "Average score per word: -5.52\n",
            "\n",
            "--- Final Evaluation Results (Best Hyperparameters) ---\n",
            "Total words in test set: 2000\n",
            "Correctly guessed words: 275\n",
            "Success Rate: 13.75%\n",
            "Total score: -11040\n",
            "Average score per word: -5.52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5dd574e"
      },
      "source": [
        "## üèÅ Final Evaluation with Best Hyperparameters\n",
        "\n",
        "### üéØ Subtask:\n",
        "Run a final evaluation with the agent trained on the **best-performing hyperparameters** identified during tuning to validate the final model‚Äôs performance.\n",
        "\n",
        "### üß† Reasoning:\n",
        "After analyzing all tuning results, the agent should be retrained and tested using the **optimal hyperparameter configuration**.  \n",
        "This step ensures that the final agent achieves **maximum success rate and stable performance** across the test dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a6ee281",
        "outputId": "1f523740-5b8a-428f-83ff-6917be0717e7"
      },
      "source": [
        "best_hyperparameters = {\n",
        "    'buffer_size': int(1e5),\n",
        "    'batch_size': 64,\n",
        "    'gamma': 0.99,\n",
        "    'lr': 5e-4,\n",
        "    'update_every': 4,\n",
        "    'epsilon_decay': 0.995,\n",
        "    'epsilon_min': 0.01,\n",
        "    'fc1_units': 128,\n",
        "    'fc2_units': 128,\n",
        "    'n_episodes': 15000,\n",
        "    'target_update_freq': 100\n",
        "}\n",
        "\n",
        "print(\"--- Running Final Evaluation with Best Hyperparameters ---\")\n",
        "print(\"Best Hyperparameters:\", best_hyperparameters)\n",
        "\n",
        "max_word_length = max(corpus_by_length.keys()) if corpus_by_length else 24\n",
        "\n",
        "final_evaluation_results = train_and_evaluate(\n",
        "    best_hyperparameters,\n",
        "    corpus_by_length,\n",
        "    hmm_probabilities,\n",
        "    test_data,\n",
        "    max_word_length\n",
        ")\n",
        "\n",
        "print(\"\\n--- Final Evaluation Results (Best Hyperparameters) ---\")\n",
        "print(f\"Total words in test set: {final_evaluation_results['total_words']}\")\n",
        "print(f\"Correctly guessed words: {final_evaluation_results['correctly_guessed']}\")\n",
        "print(f\"Success Rate: {final_evaluation_results['success_rate']:.2f}%\")\n",
        "print(f\"Total score: {final_evaluation_results['total_score']}\")\n",
        "print(f\"Average score per word: {final_evaluation_results['average_score']:.2f}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Running Final Evaluation with Best Hyperparameters ---\n",
            "Best Hyperparameters: {'buffer_size': 100000, 'batch_size': 64, 'gamma': 0.99, 'lr': 0.0005, 'update_every': 4, 'epsilon_decay': 0.995, 'epsilon_min': 0.01, 'fc1_units': 128, 'fc2_units': 128, 'n_episodes': 15000, 'target_update_freq': 100}\n",
            "Episode 1000/15000, Average Score: -0.01, Epsilon: 0.01\n",
            "Episode 2000/15000, Average Score: 0.94, Epsilon: 0.01\n",
            "Episode 3000/15000, Average Score: 0.86, Epsilon: 0.01\n",
            "Episode 4000/15000, Average Score: 1.17, Epsilon: 0.01\n",
            "Episode 5000/15000, Average Score: 1.28, Epsilon: 0.01\n",
            "Episode 6000/15000, Average Score: 1.48, Epsilon: 0.01\n",
            "Episode 7000/15000, Average Score: 1.84, Epsilon: 0.01\n",
            "Episode 8000/15000, Average Score: 1.97, Epsilon: 0.01\n",
            "Episode 9000/15000, Average Score: 2.13, Epsilon: 0.01\n",
            "Episode 10000/15000, Average Score: 2.19, Epsilon: 0.01\n",
            "Episode 11000/15000, Average Score: 2.15, Epsilon: 0.01\n",
            "Episode 12000/15000, Average Score: 2.46, Epsilon: 0.01\n",
            "Episode 13000/15000, Average Score: 2.42, Epsilon: 0.01\n",
            "Episode 14000/15000, Average Score: 2.28, Epsilon: 0.01\n",
            "Episode 15000/15000, Average Score: 2.43, Epsilon: 0.01\n",
            "\n",
            "Starting evaluation on the test set...\n",
            "\n",
            "Evaluation finished.\n",
            "Correctly guessed words: 307\n",
            "Success Rate: 15.35%\n",
            "Total score: -10040\n",
            "Average score per word: -5.02\n",
            "\n",
            "--- Final Evaluation Results (Best Hyperparameters) ---\n",
            "Total words in test set: 2000\n",
            "Correctly guessed words: 307\n",
            "Success Rate: 15.35%\n",
            "Total score: -10040\n",
            "Average score per word: -5.02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0feeeb16"
      },
      "source": [
        "## üß© Analyze Results and Select Best Hyperparameters\n",
        "\n",
        "### üîπ Subtask:\n",
        "Select the best hyperparameter configuration based on success rate and average score.\n",
        "\n",
        "### üí° Reasoning:\n",
        "Identify the set of hyperparameters that achieved the highest performance metrics from the evaluation results.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "788c1148",
        "outputId": "3ff48a0b-9e59-48cf-e416-7380c2e77be5"
      },
      "source": [
        "best_success_rate = -1\n",
        "best_avg_score = -float('inf')\n",
        "best_hyperparameters = None\n",
        "best_results = None\n",
        "\n",
        "print(\"Analyzing results to find the best hyperparameters...\")\n",
        "for result_entry in evaluation_results_list:\n",
        "    hyperparameters = result_entry['hyperparameters']\n",
        "    results = result_entry['results']\n",
        "    success_rate = results['success_rate']\n",
        "    average_score = results['average_score']\n",
        "\n",
        "    print(f\"\\nChecking Hyperparameters: {hyperparameters}\")\n",
        "    print(f\"  Success Rate: {success_rate:.2f}%\")\n",
        "    print(f\"  Average Score: {average_score:.2f}\")\n",
        "\n",
        "    if success_rate > best_success_rate:\n",
        "        best_success_rate = success_rate\n",
        "        best_avg_score = average_score\n",
        "        best_hyperparameters = hyperparameters\n",
        "        best_results = results\n",
        "    elif success_rate == best_success_rate and average_score > best_avg_score:\n",
        "        best_avg_score = average_score\n",
        "        best_hyperparameters = hyperparameters\n",
        "        best_results = results\n",
        "\n",
        "print(\"\\n--- Best Hyperparameters Found ---\")\n",
        "if best_hyperparameters:\n",
        "    print(\"Hyperparameters:\", best_hyperparameters)\n",
        "    print(\"Evaluation Results:\", best_results)\n",
        "else:\n",
        "    print(\"No evaluation results found.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing results to find the best hyperparameters...\n",
            "\n",
            "Checking Hyperparameters: {'buffer_size': 100000, 'batch_size': 64, 'gamma': 0.99, 'lr': 0.0005, 'update_every': 4, 'epsilon_decay': 0.995, 'epsilon_min': 0.01, 'fc1_units': 128, 'fc2_units': 128, 'n_episodes': 5000, 'target_update_freq': 100}\n",
            "  Success Rate: 13.75%\n",
            "  Average Score: -5.52\n",
            "\n",
            "Checking Hyperparameters: {'buffer_size': 50000, 'batch_size': 32, 'gamma': 0.95, 'lr': 0.001, 'update_every': 10, 'epsilon_decay': 0.99, 'epsilon_min': 0.05, 'fc1_units': 64, 'fc2_units': 64, 'n_episodes': 5000, 'target_update_freq': 50}\n",
            "  Success Rate: 9.60%\n",
            "  Average Score: -6.63\n",
            "\n",
            "Checking Hyperparameters: {'buffer_size': 200000, 'batch_size': 128, 'gamma': 0.999, 'lr': 0.0001, 'update_every': 1, 'epsilon_decay': 0.998, 'epsilon_min': 0.005, 'fc1_units': 256, 'fc2_units': 256, 'n_episodes': 5000, 'target_update_freq': 200}\n",
            "  Success Rate: 10.45%\n",
            "  Average Score: -6.83\n",
            "\n",
            "--- Best Hyperparameters Found ---\n",
            "Hyperparameters: {'buffer_size': 100000, 'batch_size': 64, 'gamma': 0.99, 'lr': 0.0005, 'update_every': 4, 'epsilon_decay': 0.995, 'epsilon_min': 0.01, 'fc1_units': 128, 'fc2_units': 128, 'n_episodes': 5000, 'target_update_freq': 100}\n",
            "Evaluation Results: {'total_words': 2000, 'correctly_guessed': 275, 'success_rate': 13.750000000000002, 'total_score': -11040, 'average_score': -5.52}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb9c5ec2"
      },
      "source": [
        "## üîß Define Hyperparameter Search Space\n",
        "\n",
        "### üß© Subtask:\n",
        "Specify the range of potential values for each key hyperparameter that influences the DQN agent‚Äôs performance.\n",
        "\n",
        "**Reasoning:**\n",
        "Establish a collection of hyperparameter dictionaries to explore during tuning.  \n",
        "These combinations will represent different configurations of learning rate, discount factor, batch size, and other parameters ‚Äî enabling effective experimentation through random or manual search.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcb07a16",
        "outputId": "7f5c25c9-acbf-4e6b-c61a-39d85bdd9270"
      },
      "source": [
        "hyperparameter_combinations = [\n",
        "    {\n",
        "        'buffer_size': int(1e5),\n",
        "        'batch_size': 64,\n",
        "        'gamma': 0.99,\n",
        "        'lr': 5e-4,\n",
        "        'update_every': 4,\n",
        "        'epsilon_decay': 0.995,\n",
        "        'epsilon_min': 0.01,\n",
        "        'fc1_units': 64,\n",
        "        'fc2_units': 64,\n",
        "        'n_episodes': 10000,\n",
        "        'target_update_freq': 100\n",
        "    },\n",
        "    {\n",
        "        'buffer_size': int(5e4),\n",
        "        'batch_size': 32,\n",
        "        'gamma': 0.95,\n",
        "        'lr': 1e-3,\n",
        "        'update_every': 10,\n",
        "        'epsilon_decay': 0.99,\n",
        "        'epsilon_min': 0.05,\n",
        "        'fc1_units': 64,\n",
        "        'fc2_units': 64,\n",
        "        'n_episodes': 5000,\n",
        "        'target_update_freq': 50\n",
        "    },\n",
        "    {\n",
        "        'buffer_size': int(2e5),\n",
        "        'batch_size': 128,\n",
        "        'gamma': 0.999,\n",
        "        'lr': 1e-4,\n",
        "        'update_every': 1,\n",
        "        'epsilon_decay': 0.998,\n",
        "        'epsilon_min': 0.005,\n",
        "        'fc1_units': 128,\n",
        "        'fc2_units': 128,\n",
        "        'n_episodes': 5000,\n",
        "        'target_update_freq': 200\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"Defined hyperparameter combinations for tuning:\")\n",
        "for i, hp_set in enumerate(hyperparameter_combinations):\n",
        "    print(f\"Set {i+1}: {hp_set}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined hyperparameter combinations for tuning:\n",
            "Set 1: {'buffer_size': 100000, 'batch_size': 64, 'gamma': 0.99, 'lr': 0.0005, 'update_every': 4, 'epsilon_decay': 0.995, 'epsilon_min': 0.01, 'fc1_units': 64, 'fc2_units': 64, 'n_episodes': 10000, 'target_update_freq': 100}\n",
            "Set 2: {'buffer_size': 50000, 'batch_size': 32, 'gamma': 0.95, 'lr': 0.001, 'update_every': 10, 'epsilon_decay': 0.99, 'epsilon_min': 0.05, 'fc1_units': 64, 'fc2_units': 64, 'n_episodes': 5000, 'target_update_freq': 50}\n",
            "Set 3: {'buffer_size': 200000, 'batch_size': 128, 'gamma': 0.999, 'lr': 0.0001, 'update_every': 1, 'epsilon_decay': 0.998, 'epsilon_min': 0.005, 'fc1_units': 128, 'fc2_units': 128, 'n_episodes': 5000, 'target_update_freq': 200}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "680767cf"
      },
      "source": [
        "## üîç Run Hyperparameter Tuning  \n",
        "\n",
        "### üéØ Subtask:  \n",
        "Train and evaluate the agent across multiple hyperparameter combinations to identify the best-performing setup.  \n",
        "\n",
        "**üß† Reasoning:**  \n",
        "Loop through the defined hyperparameter sets, train the agent for each configuration, and record the per\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c491635",
        "outputId": "439830f8-e498-4499-cc02-4b01d453ba50"
      },
      "source": [
        "evaluation_results_list = []\n",
        "\n",
        "max_word_length = max(corpus_by_length.keys()) if corpus_by_length else 24\n",
        "\n",
        "for i, hp_set in enumerate(hyperparameter_combinations):\n",
        "    print(f\"\\n--- Training and evaluating with Hyperparameter Set {i+1} ---\")\n",
        "    print(\"Hyperparameters:\", hp_set)\n",
        "    results = train_and_evaluate(hp_set, corpus_by_length, hmm_probabilities, test_data, max_word_length)\n",
        "    evaluation_results_list.append({'hyperparameters': hp_set, 'results': results})\n",
        "\n",
        "print(\"\\n--- Summary of Hyperparameter Tuning Results ---\")\n",
        "for result_entry in evaluation_results_list:\n",
        "    print(\"\\nHyperparameters:\", result_entry['hyperparameters'])\n",
        "    print(\"Evaluation Results:\", result_entry['results'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training and evaluating with Hyperparameter Set 1 ---\n",
            "Hyperparameters: {'buffer_size': 100000, 'batch_size': 64, 'gamma': 0.99, 'lr': 0.0005, 'update_every': 4, 'epsilon_decay': 0.995, 'epsilon_min': 0.01, 'fc1_units': 64, 'fc2_units': 64, 'n_episodes': 10000, 'target_update_freq': 100}\n",
            "Episode 1000/10000, Average Score: -0.56, Epsilon: 0.01\n",
            "Episode 2000/10000, Average Score: 1.29, Epsilon: 0.01\n",
            "Episode 3000/10000, Average Score: 1.98, Epsilon: 0.01\n",
            "Episode 4000/10000, Average Score: 1.98, Epsilon: 0.01\n",
            "Episode 5000/10000, Average Score: 1.36, Epsilon: 0.01\n",
            "Episode 6000/10000, Average Score: 2.01, Epsilon: 0.01\n",
            "Episode 7000/10000, Average Score: 1.42, Epsilon: 0.01\n",
            "Episode 8000/10000, Average Score: 2.44, Epsilon: 0.01\n",
            "Episode 9000/10000, Average Score: 1.71, Epsilon: 0.01\n",
            "Episode 10000/10000, Average Score: 1.61, Epsilon: 0.01\n",
            "\n",
            "Starting evaluation on the test set...\n",
            "\n",
            "Evaluation finished.\n",
            "Correctly guessed words: 315\n",
            "Success Rate: 15.75%\n",
            "Total score: -9990\n",
            "Average score per word: -5.00\n",
            "\n",
            "--- Training and evaluating with Hyperparameter Set 2 ---\n",
            "Hyperparameters: {'buffer_size': 50000, 'batch_size': 32, 'gamma': 0.95, 'lr': 0.001, 'update_every': 10, 'epsilon_decay': 0.99, 'epsilon_min': 0.05, 'fc1_units': 64, 'fc2_units': 64, 'n_episodes': 5000, 'target_update_freq': 50}\n",
            "Episode 1000/5000, Average Score: -1.01, Epsilon: 0.05\n",
            "Episode 2000/5000, Average Score: 0.15, Epsilon: 0.05\n",
            "Episode 3000/5000, Average Score: 0.92, Epsilon: 0.05\n",
            "Episode 4000/5000, Average Score: 0.82, Epsilon: 0.05\n",
            "Episode 5000/5000, Average Score: 0.57, Epsilon: 0.05\n",
            "\n",
            "Starting evaluation on the test set...\n",
            "\n",
            "Evaluation finished.\n",
            "Correctly guessed words: 192\n",
            "Success Rate: 9.60%\n",
            "Total score: -13270\n",
            "Average score per word: -6.63\n",
            "\n",
            "--- Training and evaluating with Hyperparameter Set 3 ---\n",
            "Hyperparameters: {'buffer_size': 200000, 'batch_size': 128, 'gamma': 0.999, 'lr': 0.0001, 'update_every': 1, 'epsilon_decay': 0.998, 'epsilon_min': 0.005, 'fc1_units': 128, 'fc2_units': 128, 'n_episodes': 5000, 'target_update_freq': 200}\n",
            "Episode 1000/5000, Average Score: -0.52, Epsilon: 0.01\n",
            "Episode 2000/5000, Average Score: 0.91, Epsilon: 0.01\n",
            "Episode 3000/5000, Average Score: 0.18, Epsilon: 0.01\n",
            "Episode 4000/5000, Average Score: 1.20, Epsilon: 0.01\n",
            "Episode 5000/5000, Average Score: 1.47, Epsilon: 0.01\n",
            "\n",
            "Starting evaluation on the test set...\n",
            "\n",
            "Evaluation finished.\n",
            "Correctly guessed words: 245\n",
            "Success Rate: 12.25%\n",
            "Total score: -11990\n",
            "Average score per word: -6.00\n",
            "\n",
            "--- Summary of Hyperparameter Tuning Results ---\n",
            "\n",
            "Hyperparameters: {'buffer_size': 100000, 'batch_size': 64, 'gamma': 0.99, 'lr': 0.0005, 'update_every': 4, 'epsilon_decay': 0.995, 'epsilon_min': 0.01, 'fc1_units': 64, 'fc2_units': 64, 'n_episodes': 10000, 'target_update_freq': 100}\n",
            "Evaluation Results: {'total_words': 2000, 'correctly_guessed': 315, 'success_rate': 15.75, 'total_score': -9990, 'average_score': -4.995}\n",
            "\n",
            "Hyperparameters: {'buffer_size': 50000, 'batch_size': 32, 'gamma': 0.95, 'lr': 0.001, 'update_every': 10, 'epsilon_decay': 0.99, 'epsilon_min': 0.05, 'fc1_units': 64, 'fc2_units': 64, 'n_episodes': 5000, 'target_update_freq': 50}\n",
            "Evaluation Results: {'total_words': 2000, 'correctly_guessed': 192, 'success_rate': 9.6, 'total_score': -13270, 'average_score': -6.635}\n",
            "\n",
            "Hyperparameters: {'buffer_size': 200000, 'batch_size': 128, 'gamma': 0.999, 'lr': 0.0001, 'update_every': 1, 'epsilon_decay': 0.998, 'epsilon_min': 0.005, 'fc1_units': 128, 'fc2_units': 128, 'n_episodes': 5000, 'target_update_freq': 200}\n",
            "Evaluation Results: {'total_words': 2000, 'correctly_guessed': 245, 'success_rate': 12.25, 'total_score': -11990, 'average_score': -5.995}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7409d6d4",
        "outputId": "d233e74b-b502-4581-e1dd-4470504e0be0"
      },
      "source": [
        "evaluation_results_list = []\n",
        "\n",
        "max_word_length = max(corpus_by_length.keys()) if corpus_by_length else 24\n",
        "\n",
        "hyperparameter_combinations = [\n",
        "    {\n",
        "        'n_episodes': 50000,\n",
        "        'target_update_freq': 500,\n",
        "        'fc1_units': 256,\n",
        "        'fc2_units': 256,\n",
        "        'fc3_units': 128,\n",
        "        'buffer_size': int(5e5),\n",
        "        'batch_size': 128,\n",
        "        'gamma': 0.99,\n",
        "        'lr': 1e-4,\n",
        "        'update_every': 4,\n",
        "        'epsilon_start': 1.0,\n",
        "        'epsilon_decay': 0.9995,\n",
        "        'epsilon_min': 0.05,\n",
        "        'hmm_weight': 0.4,\n",
        "        'rl_weight': 0.6,\n",
        "        'correct_letter_reward': 2.0,\n",
        "        'incorrect_letter_reward': -1.5,\n",
        "        'win_reward': 20.0,\n",
        "        'lose_reward': -15.0,\n",
        "        'repeated_letter_penalty': -2.0,\n",
        "    }\n",
        "]\n",
        "\n",
        "for i, hp_set in enumerate(hyperparameter_combinations):\n",
        "    print(f\"\\n--- Training and evaluating with Hyperparameter Set {i+1} ---\")\n",
        "    print(\"Hyperparameters:\", hp_set)\n",
        "    results = train_and_evaluate(hp_set, corpus_by_length, hmm_probabilities, test_data, max_word_length)\n",
        "    evaluation_results_list.append({'hyperparameters': hp_set, 'results': results})\n",
        "\n",
        "print(\"\\n--- Summary of Hyperparameter Tuning Results ---\")\n",
        "for result_entry in evaluation_results_list:\n",
        "    print(\"\\nHyperparameters:\", result_entry['hyperparameters'])\n",
        "    print(\"Evaluation Results:\", result_entry['results'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training and evaluating with Hyperparameter Set 1 ---\n",
            "Hyperparameters: {'n_episodes': 50000, 'target_update_freq': 500, 'fc1_units': 256, 'fc2_units': 256, 'fc3_units': 128, 'buffer_size': 500000, 'batch_size': 128, 'gamma': 0.99, 'lr': 0.0001, 'update_every': 4, 'epsilon_start': 1.0, 'epsilon_decay': 0.9995, 'epsilon_min': 0.05, 'hmm_weight': 0.4, 'rl_weight': 0.6, 'correct_letter_reward': 2.0, 'incorrect_letter_reward': -1.5, 'win_reward': 20.0, 'lose_reward': -15.0, 'repeated_letter_penalty': -2.0}\n",
            "Episode 1000/50000, Average Score: -13.69, Epsilon: 0.6065\n",
            "Episode 2000/50000, Average Score: -8.41, Epsilon: 0.3678\n",
            "Episode 3000/50000, Average Score: -4.12, Epsilon: 0.2230\n",
            "Episode 4000/50000, Average Score: -1.41, Epsilon: 0.1353\n",
            "Episode 5000/50000, Average Score: 2.22, Epsilon: 0.0820\n",
            "Episode 6000/50000, Average Score: 2.69, Epsilon: 0.0500\n",
            "Episode 7000/50000, Average Score: 4.20, Epsilon: 0.0500\n",
            "Episode 8000/50000, Average Score: 3.67, Epsilon: 0.0500\n",
            "Episode 9000/50000, Average Score: 3.15, Epsilon: 0.0500\n",
            "Episode 10000/50000, Average Score: 5.89, Epsilon: 0.0500\n",
            "Episode 11000/50000, Average Score: 4.70, Epsilon: 0.0500\n",
            "Episode 12000/50000, Average Score: 5.92, Epsilon: 0.0500\n",
            "Episode 13000/50000, Average Score: 5.79, Epsilon: 0.0500\n",
            "Episode 14000/50000, Average Score: 5.41, Epsilon: 0.0500\n",
            "Episode 15000/50000, Average Score: 5.90, Epsilon: 0.0500\n",
            "Episode 16000/50000, Average Score: 5.77, Epsilon: 0.0500\n",
            "Episode 17000/50000, Average Score: 6.42, Epsilon: 0.0500\n",
            "Episode 18000/50000, Average Score: 6.94, Epsilon: 0.0500\n",
            "Episode 19000/50000, Average Score: 4.27, Epsilon: 0.0500\n",
            "Episode 20000/50000, Average Score: 4.96, Epsilon: 0.0500\n",
            "Episode 21000/50000, Average Score: 5.01, Epsilon: 0.0500\n",
            "Episode 22000/50000, Average Score: 7.00, Epsilon: 0.0500\n",
            "Episode 23000/50000, Average Score: 5.38, Epsilon: 0.0500\n",
            "Episode 24000/50000, Average Score: 6.31, Epsilon: 0.0500\n",
            "Episode 25000/50000, Average Score: 6.57, Epsilon: 0.0500\n",
            "Episode 26000/50000, Average Score: 7.30, Epsilon: 0.0500\n",
            "Episode 27000/50000, Average Score: 6.29, Epsilon: 0.0500\n",
            "Episode 28000/50000, Average Score: 7.53, Epsilon: 0.0500\n",
            "Episode 29000/50000, Average Score: 7.93, Epsilon: 0.0500\n",
            "Episode 30000/50000, Average Score: 8.00, Epsilon: 0.0500\n",
            "Episode 31000/50000, Average Score: 6.63, Epsilon: 0.0500\n",
            "Episode 32000/50000, Average Score: 7.17, Epsilon: 0.0500\n",
            "Episode 33000/50000, Average Score: 6.57, Epsilon: 0.0500\n",
            "Episode 34000/50000, Average Score: 7.11, Epsilon: 0.0500\n",
            "Episode 35000/50000, Average Score: 8.19, Epsilon: 0.0500\n",
            "Episode 36000/50000, Average Score: 8.65, Epsilon: 0.0500\n",
            "Episode 37000/50000, Average Score: 7.14, Epsilon: 0.0500\n",
            "Episode 38000/50000, Average Score: 6.84, Epsilon: 0.0500\n",
            "Episode 39000/50000, Average Score: 8.27, Epsilon: 0.0500\n",
            "Episode 40000/50000, Average Score: 7.29, Epsilon: 0.0500\n",
            "Episode 41000/50000, Average Score: 7.20, Epsilon: 0.0500\n",
            "Episode 42000/50000, Average Score: 6.95, Epsilon: 0.0500\n",
            "Episode 43000/50000, Average Score: 8.21, Epsilon: 0.0500\n",
            "Episode 44000/50000, Average Score: 8.12, Epsilon: 0.0500\n",
            "Episode 45000/50000, Average Score: 9.22, Epsilon: 0.0500\n",
            "Episode 46000/50000, Average Score: 8.19, Epsilon: 0.0500\n",
            "Episode 47000/50000, Average Score: 8.98, Epsilon: 0.0500\n",
            "Episode 48000/50000, Average Score: 8.95, Epsilon: 0.0500\n",
            "Episode 49000/50000, Average Score: 9.44, Epsilon: 0.0500\n",
            "Episode 50000/50000, Average Score: 8.13, Epsilon: 0.0500\n",
            "\n",
            "Starting evaluation on the test set...\n",
            "\n",
            "Evaluation finished.\n",
            "Correctly guessed words: 362\n",
            "Success Rate: 18.10%\n",
            "Total score: -8030\n",
            "Average score per word: -4.01\n",
            "\n",
            "--- Summary of Hyperparameter Tuning Results ---\n",
            "\n",
            "Hyperparameters: {'n_episodes': 50000, 'target_update_freq': 500, 'fc1_units': 256, 'fc2_units': 256, 'fc3_units': 128, 'buffer_size': 500000, 'batch_size': 128, 'gamma': 0.99, 'lr': 0.0001, 'update_every': 4, 'epsilon_start': 1.0, 'epsilon_decay': 0.9995, 'epsilon_min': 0.05, 'hmm_weight': 0.4, 'rl_weight': 0.6, 'correct_letter_reward': 2.0, 'incorrect_letter_reward': -1.5, 'win_reward': 20.0, 'lose_reward': -15.0, 'repeated_letter_penalty': -2.0}\n",
            "Evaluation Results: {'total_words': 2000, 'correctly_guessed': 362, 'success_rate': 18.099999999999998, 'total_score': -8030, 'average_score': -4.015}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q3Vhs7_EZcBQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}